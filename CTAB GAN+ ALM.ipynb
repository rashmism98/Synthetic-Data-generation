{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyPxawKE6CJbrCoez1k/yoyK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXlcTZGWeY5s","executionInfo":{"status":"ok","timestamp":1765227530536,"user_tz":480,"elapsed":27266,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"45687c75-ddde-46ff-ba4f-89f83b9bb40d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# then, for convenience\n","%cd /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbGQphq5fxLG","executionInfo":{"status":"ok","timestamp":1765172792819,"user_tz":480,"elapsed":206,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"72f08f7f-8f64-4bdd-ba6e-32ad079750ea"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec  8 05:46:32 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# ALM / PSyGen settings\n","alm:\n","  P_min: 0.7          # minimum acceptable privacy score\n","  lambda_init: 0.0\n","  mu_init: 0.1\n","  lambda_lr: 1e-3     # step size for λ\n","  mu_growth: 1.5      # µ ← mu_growth * µ when violations persist\n","  alm_weight: 1.0     # how strongly to weight ALM term vs adv loss\n","\n","  # quality metric weights (sum to 1)\n","  w_quality:\n","    D: 0.3   # distribution similarity\n","    A: 0.1   # anomaly / rare events\n","    C: 0.3   # inter-feature associations\n","    V: 0.2   # diversity / coverage\n","    T: 0.1   # temporal consistency (0 if not temporal data)\n","\n","  # privacy metric weights (sum to 1)\n","  w_privacy:\n","    DCR: 0.4    # distance to closest real\n","    Qdelta: 0.3 # quantile difference\n","    I: 0.3      # duplicate score\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"p8Eqlyzhf2BW","executionInfo":{"status":"error","timestamp":1765172817418,"user_tz":480,"elapsed":54,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"398b10fa-d354-40a1-e04d-7c0134ccf447"},"execution_count":2,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-4072824666.py, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4072824666.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    alm:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["# PSyGen-style ALM Tabular GAN demo for Google Colab\n","# ---------------------------------------------------\n","# This script:\n","# 1) Loads a numeric CSV dataset (or generates a toy one if not found)\n","# 2) Trains a simple WGAN-GP GAN on the table\n","# 3) Wraps the generator loss with the Augmented Lagrangian Method (ALM)\n","#    using PSyGen-style quality Q and privacy P metrics.\n","#\n","# Paste this whole cell into a Colab notebook and run it.\n","# Edit DATA_PATH to point to your real CSV.\n","\n","!pip install -q pandas scikit-learn tqdm\n","\n","import os\n","import math\n","import random\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","\n","# ------------------\n","# Config\n","# ------------------\n","\n","DATA_PATH = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"  # change to your CSV path\n","BATCH_SIZE = 256\n","EPOCHS = 50\n","LATENT_DIM = 64\n","LR_G = 1e-4\n","LR_D = 1e-4\n","N_CRITIC = 5        # WGAN-GP critic steps per G step\n","LAMBDA_GP = 10.0    # gradient penalty weight\n","\n","# ALM config\n","P_MIN = 0.7         # target minimum privacy-preservability\n","ALM_WEIGHT = 1.0    # how strongly to weight ALM term vs adversarial\n","LAMBDA_INIT = 0.0\n","MU_INIT = 0.1\n","LAMBDA_LR = 1e-3\n","MU_GROWTH = 1.5\n","\n","# Quality metric weights (sum ≈ 1)\n","WQ = dict(D=0.3, A=0.1, C=0.3, V=0.3, T=0.0)\n","\n","# Privacy metric weights (sum ≈ 1)\n","WP = dict(DCR=0.4, Qdelta=0.3, I=0.3)\n","\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", DEVICE)\n","\n","# ------------------\n","# Data\n","# ------------------\n","\n","class TabularDataset(Dataset):\n","    def __init__(self, x):\n","        self.x = x.astype(np.float32)\n","\n","    def __len__(self):\n","        return self.x.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.x[idx]\n","\n","\n","def load_or_make_data():\n","    \"\"\"\n","    Load a numeric CSV from DATA_PATH, or create a toy dataset if not found.\n","    Assumes all columns are numeric; encode categoricals beforehand if needed.\n","    \"\"\"\n","    if not os.path.exists(DATA_PATH):\n","        print(f\"{DATA_PATH} not found, generating a toy dataset instead.\")\n","        from sklearn.datasets import make_classification\n","        X, y = make_classification(\n","            n_samples=5000,\n","            n_features=8,\n","            n_informative=5,\n","            n_redundant=0,\n","            n_clusters_per_class=1,\n","            random_state=42,\n","        )\n","        df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n","        df[\"label\"] = y\n","        df.to_csv(DATA_PATH, index=False)\n","    else:\n","        print(f\"Loading data from {DATA_PATH}\")\n","\n","    df = pd.read_csv(DATA_PATH)\n","    print(\"Columns:\", df.columns.tolist())\n","    # Enforce numeric\n","    for c in df.columns:\n","        if not pd.api.types.is_numeric_dtype(df[c]):\n","            raise ValueError(\n","                f\"Column {c} is not numeric. \"\n","                \"Please encode categorical columns before using this script.\"\n","            )\n","\n","    scaler = StandardScaler()\n","    X = scaler.fit_transform(df.values.astype(np.float32))\n","\n","    return X, df.columns.tolist(), scaler\n","\n","\n","X_real, column_names, scaler = load_or_make_data()\n","dim = X_real.shape[1]\n","print(\"Data shape:\", X_real.shape)\n","\n","dataset = TabularDataset(X_real)\n","dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","\n","# ------------------\n","# Models\n","# ------------------\n","\n","class Generator(nn.Module):\n","    def __init__(self, latent_dim, data_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(latent_dim, 256),\n","            nn.ReLU(True),\n","            nn.Linear(256, 256),\n","            nn.ReLU(True),\n","            nn.Linear(256, data_dim),\n","        )\n","\n","    def forward(self, z):\n","        return self.net(z)\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, data_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(data_dim, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(256, 1),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x).view(-1)\n","\n","G = Generator(LATENT_DIM, dim).to(DEVICE)\n","D = Discriminator(dim).to(DEVICE)\n","\n","opt_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=(0.5, 0.9))\n","opt_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=(0.5, 0.9))\n","\n","# ------------------\n","# WGAN-GP helpers\n","# ------------------\n","\n","def gradient_penalty(D, real, fake):\n","    batch_size = real.size(0)\n","    alpha = torch.rand(batch_size, 1, device=real.device)\n","    alpha = alpha.expand_as(real)\n","\n","    interpolated = alpha * real + (1 - alpha) * fake\n","    interpolated.requires_grad_(True)\n","\n","    d_interpolated = D(interpolated)\n","\n","    gradients = torch.autograd.grad(\n","        outputs=d_interpolated,\n","        inputs=interpolated,\n","        grad_outputs=torch.ones_like(d_interpolated),\n","        create_graph=True,\n","        retain_graph=True,\n","        only_inputs=True,\n","    )[0]\n","\n","    gradients = gradients.view(batch_size, -1)\n","    grad_norm = gradients.norm(2, dim=1)\n","    gp = ((grad_norm - 1) ** 2).mean()\n","    return gp\n","\n","\n","def sample_noise(batch_size, latent_dim, device):\n","    return torch.randn(batch_size, latent_dim, device=device)\n","\n","# ------------------\n","# PSyGen-style Q and P metrics\n","# ------------------\n","\n","class QualityPrivacyMetrics:\n","    \"\"\"\n","    Differentiable-ish proxies for PSyGen quality Q and privacy-preservability P.\n","    Operate on scaled numeric data (real_batch, fake_batch) of shape [B, dim].\n","    \"\"\"\n","\n","    def __init__(self, w_quality, w_privacy, device=\"cuda\"):\n","        self.wq = w_quality\n","        self.wp = w_privacy\n","        self.device = device\n","\n","    def _distribution_similarity(self, real, fake):\n","        \"\"\"\n","        D: use RBF-kernel MMD and map to [0,1] via exp(-mmd2).\n","        \"\"\"\n","        def rbf(x, y, gamma=1.0):\n","            x = x.view(x.size(0), -1)\n","            y = y.view(y.size(0), -1)\n","            xx = torch.cdist(x, x, p=2)\n","            yy = torch.cdist(y, y, p=2)\n","            xy = torch.cdist(x, y, p=2)\n","            kxx = torch.exp(-gamma * xx ** 2).mean()\n","            kyy = torch.exp(-gamma * yy ** 2).mean()\n","            kxy = torch.exp(-gamma * xy ** 2).mean()\n","            return kxx + kyy - 2 * kxy\n","\n","        mmd2 = rbf(real, fake)\n","        return torch.exp(-mmd2).clamp(0.0, 1.0)\n","\n","    def _anomaly_preservation(self, real, fake):\n","        \"\"\"\n","        A: based on IQR anomalies.\n","        Not perfectly differentiable but okay as an auxiliary signal.\n","        \"\"\"\n","        q1_real = torch.quantile(real, 0.25, dim=0)\n","        q3_real = torch.quantile(real, 0.75, dim=0)\n","        iqr_real = q3_real - q1_real + 1e-6\n","        hi = q3_real + 1.5 * iqr_real\n","        lo = q1_real - 1.5 * iqr_real\n","\n","        rare_real = ((real > hi) | (real < lo)).float().mean()\n","        rare_fake = ((fake > hi) | (fake < lo)).float().mean()\n","        return (1.0 - torch.abs(rare_real - rare_fake)).clamp(0.0, 1.0)\n","\n","    def _association_preservation(self, real, fake):\n","        \"\"\"\n","        C: compare correlation matrices.\n","        \"\"\"\n","        def corr(x):\n","            x = (x - x.mean(0, keepdim=True)) / (x.std(0, keepdim=True) + 1e-6)\n","            return (x.T @ x) / (x.size(0) - 1)\n","\n","        c_real = corr(real)\n","        c_fake = corr(fake)\n","        diff = torch.mean(torch.abs(c_real - c_fake))\n","        return torch.exp(-diff).clamp(0.0, 1.0)\n","\n","    def _diversity(self, fake):\n","        \"\"\"\n","        V: encourage diversity via 1 - avg cosine similarity.\n","        \"\"\"\n","        x = fake.view(fake.size(0), -1)\n","        x = x - x.mean(0, keepdim=True)\n","        x = x / (x.norm(p=2, dim=1, keepdim=True) + 1e-6)\n","        sim = torch.mm(x, x.T)\n","        mask = ~torch.eye(sim.size(0), dtype=torch.bool, device=sim.device)\n","        avg_sim = sim[mask].mean()\n","        return (1.0 - avg_sim).clamp(0.0, 1.0)\n","\n","    def _temporal_consistency(self, real, fake):\n","        \"\"\"\n","        T: placeholder for temporal metrics (DTW etc.).\n","        For static tables we set to 0.\n","        \"\"\"\n","        return torch.tensor(0.0, device=self.device)\n","\n","    # Privacy helpers\n","    def _distance_to_closest_record(self, real, fake):\n","        \"\"\"\n","        DCR: mean min distance between fake and real.\n","        Larger distances = better privacy.\n","        Normalize via tanh.\n","        \"\"\"\n","        real = real.view(real.size(0), -1)\n","        fake = fake.view(fake.size(0), -1)\n","        dists = torch.cdist(fake, real, p=2)\n","        min_dist, _ = dists.min(dim=1)\n","        dcr_raw = min_dist.mean()\n","        dcr_norm = torch.tanh(dcr_raw)  # in (0,1)\n","        return dcr_norm\n","\n","    def _quantile_diff(self, real, fake):\n","        \"\"\"\n","        Qδ: mean absolute difference between quantiles.\n","        Smaller is better for privacy (less overfitting).\n","        We'll return a normalized version in [0,1].\n","        \"\"\"\n","        qs = torch.tensor([0.1, 0.25, 0.5, 0.75, 0.9], device=real.device)\n","        real_q = torch.quantile(real, qs, dim=0)\n","        fake_q = torch.quantile(fake, qs, dim=0)\n","        diff = torch.abs(real_q - fake_q).mean()\n","        qdelta_raw = diff\n","        qdelta_norm = qdelta_raw / (qdelta_raw + 1.0)  # in (0,1)\n","        return qdelta_norm\n","\n","    def _duplicate_score(self, real, fake):\n","        \"\"\"\n","        I: duplicates within fake and between fake and real.\n","        High duplicate rate = bad for privacy.\n","        We'll approximate via very small distances.\n","        \"\"\"\n","        x = torch.cat([real, fake], dim=0)\n","        x = x.view(x.size(0), -1)\n","        dists = torch.cdist(x, x, p=2)\n","        dup_mask = (dists < 1e-3).float()\n","        dup_mask = dup_mask - torch.eye(dists.size(0), device=dists.device)\n","        dup_rate = dup_mask.clamp(min=0).mean()  # in [0,1] roughly\n","        i_norm = dup_rate\n","        return i_norm\n","\n","    def __call__(self, real_batch, fake_batch):\n","        real = real_batch.float()\n","        fake = fake_batch.float()\n","\n","        D = self._distribution_similarity(real, fake)\n","        A = self._anomaly_preservation(real, fake)\n","        C = self._association_preservation(real, fake)\n","        V = self._diversity(fake)\n","        T = self._temporal_consistency(real, fake)\n","\n","        Q = (\n","            self.wq[\"D\"] * D +\n","            self.wq[\"A\"] * A +\n","            self.wq[\"C\"] * C +\n","            self.wq[\"V\"] * V +\n","            self.wq[\"T\"] * T\n","        )\n","\n","        DCR = self._distance_to_closest_record(real, fake)\n","        Qdelta = self._quantile_diff(real, fake)\n","        I = self._duplicate_score(real, fake)\n","\n","        # P = w1*normalize(DCR) - w2*normalize(Qδ) - w3*normalize(I)\n","        P = (\n","            self.wp[\"DCR\"] * DCR -\n","            self.wp[\"Qdelta\"] * Qdelta -\n","            self.wp[\"I\"] * I\n","        )\n","\n","        return Q, P\n","\n","\n","class ALMController:\n","    \"\"\"\n","    Implements the ALM update:\n","    L_AL(θ, λ) = (1 − Q(θ)) + λ(P_min − P(θ)) + µ/2 (P_min − P(θ))^2\n","    and two-time-scale updates of λ, µ.\n","    \"\"\"\n","\n","    def __init__(self, P_min, lambda_init, mu_init, lambda_lr, mu_growth, device=\"cuda\"):\n","        self.P_min = torch.tensor(P_min, device=device)\n","        self.lambda_lr = lambda_lr\n","        self.mu_growth = mu_growth\n","        self.device = device\n","\n","        self.lmbda = torch.tensor(lambda_init, device=device)\n","        self.mu = torch.tensor(mu_init, device=device)\n","\n","        self.residual_ma = None\n","\n","    def compute_loss(self, Q, P):\n","        \"\"\"\n","        Q, P are scalar tensors.\n","        \"\"\"\n","        residual = self.P_min - P\n","        violation = torch.relu(residual)  # only if P < P_min\n","        L = (1.0 - Q) + self.lmbda * violation + 0.5 * self.mu * violation ** 2\n","        return L, residual.detach()\n","\n","    def update_multipliers(self, residual_epoch):\n","        \"\"\"\n","        residual_epoch: scalar tensor or float, mean over the epoch.\n","        \"\"\"\n","        r = residual_epoch if torch.is_tensor(residual_epoch) else torch.tensor(residual_epoch, device=self.device)\n","\n","        if self.residual_ma is None:\n","            self.residual_ma = r\n","        else:\n","            self.residual_ma = 0.9 * self.residual_ma + 0.1 * r\n","\n","        # λ_{k+1} = max(0, λ_k + η * residual)\n","        self.lmbda = (self.lmbda + self.lambda_lr * self.residual_ma).clamp(min=0.0)\n","\n","        # If privacy violations persist, increase µ\n","        if self.residual_ma > 0:\n","            self.mu = self.mu * self.mu_growth\n","\n","\n","qp_metrics = QualityPrivacyMetrics(WQ, WP, device=DEVICE)\n","alm = ALMController(\n","    P_min=P_MIN,\n","    lambda_init=LAMBDA_INIT,\n","    mu_init=MU_INIT,\n","    lambda_lr=LAMBDA_LR,\n","    mu_growth=MU_GROWTH,\n","    device=DEVICE,\n",")\n","\n","# ------------------\n","# Training loop\n","# ------------------\n","\n","G.train()\n","D.train()\n","\n","global_step = 0\n","\n","for epoch in range(1, EPOCHS + 1):\n","    d_losses = []\n","    g_losses = []\n","    q_scores = []\n","    p_scores = []\n","    residuals = []\n","\n","    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n","    for real_batch in pbar:\n","        real_batch = real_batch.to(DEVICE)\n","\n","        # ------------------\n","        # Train Discriminator / Critic\n","        # ------------------\n","        for _ in range(N_CRITIC):\n","            z = sample_noise(real_batch.size(0), LATENT_DIM, DEVICE)\n","            fake_batch = G(z).detach()\n","\n","            d_real = D(real_batch)\n","            d_fake = D(fake_batch)\n","\n","            gp = gradient_penalty(D, real_batch, fake_batch)\n","            d_loss = -(d_real.mean() - d_fake.mean()) + LAMBDA_GP * gp\n","\n","            opt_D.zero_grad()\n","            d_loss.backward()\n","            opt_D.step()\n","\n","        # ------------------\n","        # Train Generator with ALM\n","        # ------------------\n","        z = sample_noise(real_batch.size(0), LATENT_DIM, DEVICE)\n","        fake_batch = G(z)\n","\n","        adv_loss = -D(fake_batch).mean()  # WGAN generator loss\n","\n","        Q, P = qp_metrics(real_batch, fake_batch)\n","        L_AL, residual = alm.compute_loss(Q, P)\n","\n","        g_loss = adv_loss + ALM_WEIGHT * L_AL\n","\n","        opt_G.zero_grad()\n","        g_loss.backward()\n","        opt_G.step()\n","\n","        d_losses.append(d_loss.item())\n","        g_losses.append(g_loss.item())\n","        q_scores.append(Q.item())\n","        p_scores.append(P.item())\n","        residuals.append(residual.item())\n","\n","        global_step += 1\n","        pbar.set_postfix(\n","            d_loss=f\"{np.mean(d_losses):.3f}\",\n","            g_loss=f\"{np.mean(g_losses):.3f}\",\n","            Q=f\"{np.mean(q_scores):.3f}\",\n","            P=f\"{np.mean(p_scores):.3f}\",\n","        )\n","\n","    mean_residual = float(np.mean(residuals))\n","    alm.update_multipliers(mean_residual)\n","\n","    print(\n","        f\"Epoch {epoch:03d} | \"\n","        f\"D_loss={np.mean(d_losses):.3f} | \"\n","        f\"G_loss={np.mean(g_losses):.3f} | \"\n","        f\"Q={np.mean(q_scores):.3f} | \"\n","        f\"P={np.mean(p_scores):.3f} | \"\n","        f\"residual={mean_residual:.3f} | \"\n","        f\"lambda={alm.lmbda.item():.3f} | \"\n","        f\"mu={alm.mu.item():.3f}\"\n","    )\n","\n","print(\"Training finished.\")\n","\n","# ------------------\n","# Sample synthetic data\n","# ------------------\n","\n","G.eval()\n","with torch.no_grad():\n","    N_SYN = 1000\n","    z = sample_noise(N_SYN, LATENT_DIM, DEVICE)\n","    fake = G(z).cpu().numpy()\n","\n","# Inverse scaling back to original numeric space\n","fake_unscaled = scaler.inverse_transform(fake)\n","synthetic_df = pd.DataFrame(fake_unscaled, columns=column_names)\n","\n","print(\"Synthetic sample head:\")\n","print(synthetic_df.head())\n","\n","# Save to CSV\n","out_path = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv\"\n","synthetic_df.to_csv(out_path, index=False)\n","print(f\"Synthetic data saved to {out_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhprXlbwhYbt","executionInfo":{"status":"ok","timestamp":1765215501229,"user_tz":480,"elapsed":19969,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"7276828e-50b8-454d-ba4d-e100de315b95"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading data from /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\n","Columns: ['ID', 'CT', 'UCSi', 'UCSh', 'Madh', 'SECS', 'BN', 'BC', 'NN', 'Mi', 'Class']\n","Data shape: (683, 11)\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/50:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:841: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:270.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 001 | D_loss=6.970 | G_loss=0.304 | Q=0.704 | P=0.243 | residual=0.457 | lambda=0.000 | mu=0.150\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 002 | D_loss=4.991 | G_loss=0.288 | Q=0.703 | P=0.246 | residual=0.454 | lambda=0.001 | mu=0.225\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 003 | D_loss=3.205 | G_loss=0.252 | Q=0.705 | P=0.245 | residual=0.455 | lambda=0.001 | mu=0.338\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 004 | D_loss=1.770 | G_loss=0.194 | Q=0.706 | P=0.243 | residual=0.457 | lambda=0.002 | mu=0.506\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 005 | D_loss=0.820 | G_loss=0.112 | Q=0.710 | P=0.249 | residual=0.451 | lambda=0.002 | mu=0.759\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 006 | D_loss=0.294 | G_loss=0.031 | Q=0.711 | P=0.248 | residual=0.452 | lambda=0.003 | mu=1.139\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 007 | D_loss=0.110 | G_loss=-0.037 | Q=0.705 | P=0.251 | residual=0.449 | lambda=0.003 | mu=1.709\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 008 | D_loss=0.060 | G_loss=-0.088 | Q=0.710 | P=0.249 | residual=0.451 | lambda=0.004 | mu=2.563\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 009 | D_loss=-0.035 | G_loss=-0.060 | Q=0.706 | P=0.246 | residual=0.454 | lambda=0.004 | mu=3.844\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 010 | D_loss=-0.130 | G_loss=0.002 | Q=0.708 | P=0.249 | residual=0.451 | lambda=0.005 | mu=5.767\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 011 | D_loss=-0.217 | G_loss=0.133 | Q=0.704 | P=0.254 | residual=0.446 | lambda=0.005 | mu=8.650\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 012 | D_loss=-0.402 | G_loss=0.402 | Q=0.707 | P=0.250 | residual=0.450 | lambda=0.005 | mu=12.975\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 013 | D_loss=-0.508 | G_loss=0.787 | Q=0.709 | P=0.254 | residual=0.446 | lambda=0.006 | mu=19.462\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 014 | D_loss=-0.721 | G_loss=1.453 | Q=0.708 | P=0.250 | residual=0.450 | lambda=0.006 | mu=29.193\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 015 | D_loss=-0.809 | G_loss=2.375 | Q=0.710 | P=0.256 | residual=0.444 | lambda=0.007 | mu=43.789\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 016 | D_loss=-1.120 | G_loss=3.920 | Q=0.711 | P=0.252 | residual=0.448 | lambda=0.007 | mu=65.684\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 017 | D_loss=-1.320 | G_loss=6.122 | Q=0.710 | P=0.253 | residual=0.447 | lambda=0.008 | mu=98.526\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 018 | D_loss=-1.431 | G_loss=9.373 | Q=0.712 | P=0.256 | residual=0.444 | lambda=0.008 | mu=147.789\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 019 | D_loss=-1.682 | G_loss=14.470 | Q=0.714 | P=0.253 | residual=0.447 | lambda=0.009 | mu=221.684\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 020 | D_loss=-1.865 | G_loss=21.952 | Q=0.710 | P=0.254 | residual=0.446 | lambda=0.009 | mu=332.526\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 021 | D_loss=-1.975 | G_loss=33.002 | Q=0.711 | P=0.254 | residual=0.446 | lambda=0.010 | mu=498.789\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 022 | D_loss=-2.151 | G_loss=49.084 | Q=0.719 | P=0.257 | residual=0.443 | lambda=0.010 | mu=748.183\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 023 | D_loss=-2.407 | G_loss=73.784 | Q=0.717 | P=0.257 | residual=0.443 | lambda=0.010 | mu=1122.274\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 024 | D_loss=-2.550 | G_loss=111.425 | Q=0.714 | P=0.255 | residual=0.445 | lambda=0.011 | mu=1683.411\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 025 | D_loss=-2.734 | G_loss=166.396 | Q=0.718 | P=0.256 | residual=0.444 | lambda=0.011 | mu=2525.117\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 026 | D_loss=-2.831 | G_loss=248.782 | Q=0.716 | P=0.257 | residual=0.443 | lambda=0.012 | mu=3787.675\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 027 | D_loss=-2.895 | G_loss=372.460 | Q=0.721 | P=0.257 | residual=0.443 | lambda=0.012 | mu=5681.513\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 028 | D_loss=-2.966 | G_loss=558.450 | Q=0.721 | P=0.257 | residual=0.443 | lambda=0.013 | mu=8522.270\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 029 | D_loss=-2.940 | G_loss=829.983 | Q=0.717 | P=0.259 | residual=0.441 | lambda=0.013 | mu=12783.404\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 030 | D_loss=-2.989 | G_loss=1250.530 | Q=0.724 | P=0.258 | residual=0.442 | lambda=0.014 | mu=19175.105\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 031 | D_loss=-2.906 | G_loss=1834.045 | Q=0.725 | P=0.263 | residual=0.437 | lambda=0.014 | mu=28762.658\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 032 | D_loss=-3.020 | G_loss=2779.438 | Q=0.719 | P=0.260 | residual=0.440 | lambda=0.014 | mu=43143.988\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 033 | D_loss=-3.031 | G_loss=4172.299 | Q=0.724 | P=0.260 | residual=0.440 | lambda=0.015 | mu=64715.984\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 034 | D_loss=-3.020 | G_loss=6166.721 | Q=0.725 | P=0.263 | residual=0.437 | lambda=0.015 | mu=97073.977\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 035 | D_loss=-2.986 | G_loss=9282.730 | Q=0.717 | P=0.263 | residual=0.437 | lambda=0.016 | mu=145610.969\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 036 | D_loss=-3.091 | G_loss=14029.398 | Q=0.724 | P=0.261 | residual=0.439 | lambda=0.016 | mu=218416.453\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 037 | D_loss=-3.046 | G_loss=21020.895 | Q=0.722 | P=0.261 | residual=0.439 | lambda=0.017 | mu=327624.688\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 038 | D_loss=-3.040 | G_loss=31215.221 | Q=0.721 | P=0.263 | residual=0.437 | lambda=0.017 | mu=491437.031\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 039 | D_loss=-3.135 | G_loss=46469.994 | Q=0.718 | P=0.265 | residual=0.435 | lambda=0.018 | mu=737155.562\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 040 | D_loss=-3.144 | G_loss=70084.250 | Q=0.726 | P=0.264 | residual=0.436 | lambda=0.018 | mu=1105733.375\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 041 | D_loss=-3.233 | G_loss=105838.172 | Q=0.726 | P=0.262 | residual=0.438 | lambda=0.018 | mu=1658600.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 042 | D_loss=-3.250 | G_loss=158456.945 | Q=0.729 | P=0.263 | residual=0.437 | lambda=0.019 | mu=2487900.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 043 | D_loss=-3.203 | G_loss=236569.766 | Q=0.725 | P=0.264 | residual=0.436 | lambda=0.019 | mu=3731850.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 044 | D_loss=-3.235 | G_loss=353253.656 | Q=0.729 | P=0.265 | residual=0.435 | lambda=0.020 | mu=5597775.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 045 | D_loss=-3.196 | G_loss=528894.641 | Q=0.730 | P=0.265 | residual=0.435 | lambda=0.020 | mu=8396662.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 046 | D_loss=-3.230 | G_loss=794396.375 | Q=0.723 | P=0.265 | residual=0.435 | lambda=0.021 | mu=12594993.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 047 | D_loss=-3.267 | G_loss=1189262.562 | Q=0.731 | P=0.265 | residual=0.435 | lambda=0.021 | mu=18892490.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 048 | D_loss=-3.241 | G_loss=1777344.062 | Q=0.727 | P=0.266 | residual=0.434 | lambda=0.021 | mu=28338736.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 049 | D_loss=-3.277 | G_loss=2700718.375 | Q=0.725 | P=0.263 | residual=0.437 | lambda=0.022 | mu=42508104.000\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 050 | D_loss=-3.310 | G_loss=4016132.250 | Q=0.730 | P=0.265 | residual=0.435 | lambda=0.022 | mu=63762156.000\n","Training finished.\n","Synthetic sample head:\n","             ID        CT      UCSi      UCSh      Madh      SECS        BN  \\\n","0  9.802956e+05  3.494788  4.893621  1.101263  2.211968  1.836290  8.120708   \n","1  9.846604e+05  1.747692  5.169800  1.946902  3.232622  2.012821  7.650734   \n","2  1.047130e+06  2.965686  5.934578  1.142742  2.804155  1.559520  9.980186   \n","3  1.274527e+06  2.480464  4.493620  1.830411  2.117072  1.691466  6.905674   \n","4  1.187850e+06  3.587873  4.591006  2.031217  1.882766  1.571781  7.535800   \n","\n","         BC        NN        Mi     Class  \n","0  2.221941  2.962717  0.944879  0.511762  \n","1  4.179457  2.985354  1.013454  0.594624  \n","2  3.424134  3.791218  0.893769  0.545127  \n","3  3.024504  3.311614  0.926361  0.588189  \n","4  2.612856  3.146319  1.173965  0.466975  \n","Synthetic data saved to /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv\n"]}]},{"cell_type":"code","source":["# -----------------------------------------------------------\n","# PSyGen-style Quality (Q) and Privacy (P) metrics\n","# for real_data.csv vs synthetic_data_alm.csv\n","# -----------------------------------------------------------\n","# Assumes:\n","#   - /content/real_data.csv      (used for training)\n","#   - /content/synthetic_data_alm.csv  (produced by the GAN cell)\n","#\n","# Metrics follow the paper's structure:\n","#   Quality Q = w1*D + w2*A + w3*C + w4*V + w5*T\n","#   Privacy P = w1*normalize(DCR) - w2*normalize(Qδ) - w3*normalize(I)\n","#\n","# This implementation is for NUMERIC-ONLY data.\n","# -----------------------------------------------------------\n","\n","!pip install -q scipy\n","\n","import numpy as np\n","import pandas as pd\n","\n","from scipy.stats import ks_2samp, entropy\n","from scipy.spatial.distance import jensenshannon\n","\n","# ------------------\n","# Config: weights\n","# ------------------\n","\n","WQ = dict(D=0.3, A=0.1, C=0.3, V=0.3, T=0.0)  # quality weights (sum to 1)\n","WP = dict(DCR=0.4, Qdelta=0.3, I=0.3)         # privacy weights (sum to 1)\n","\n","REAL_PATH = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"\n","SYN_PATH  = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast_cancer_CTABGAN_DP_QP_report.csv\"\n","\n","# ------------------\n","# Utility helpers\n","# ------------------\n","\n","def align_real_synth(real_df, syn_df, random_state=42):\n","    \"\"\"\n","    Align columns and sample same number of rows from real and synthetic.\n","    Assumes the same schema (same column names / order).\n","    \"\"\"\n","    # Keep only common columns, in real's order\n","    common_cols = [c for c in real_df.columns if c in syn_df.columns]\n","    real_df = real_df[common_cols].copy()\n","    syn_df  = syn_df[common_cols].copy()\n","\n","    n = min(len(real_df), len(syn_df))\n","    rng = np.random.default_rng(random_state)\n","    real_idx = rng.choice(len(real_df), n, replace=False)\n","    syn_idx  = rng.choice(len(syn_df), n, replace=False)\n","\n","    real_df = real_df.iloc[real_idx].reset_index(drop=True)\n","    syn_df  = syn_df.iloc[syn_idx].reset_index(drop=True)\n","    return real_df, syn_df\n","\n","def hist_match(real_col, syn_col, bins=20):\n","    \"\"\"\n","    Make histograms for real and synthetic with same bin edges.\n","    Returns normalized histograms p, q (summing to 1).\n","    \"\"\"\n","    real_col = np.asarray(real_col)\n","    syn_col  = np.asarray(syn_col)\n","\n","    # Use real-data range\n","    r_min, r_max = np.nanmin(real_col), np.nanmax(real_col)\n","    if r_min == r_max:\n","        # Degenerate column: all values identical\n","        p = np.array([1.0])\n","        q = np.array([1.0])\n","        return p, q\n","\n","    hist_r, bin_edges = np.histogram(real_col, bins=bins, range=(r_min, r_max), density=False)\n","    hist_s, _        = np.histogram(syn_col,  bins=bin_edges, density=False)\n","\n","    # Add small epsilon, then normalize to probability distributions\n","    eps = 1e-12\n","    p = hist_r.astype(float) + eps\n","    q = hist_s.astype(float) + eps\n","    p /= p.sum()\n","    q /= q.sum()\n","    return p, q\n","\n","def safe_entropy(p, q):\n","    \"\"\"\n","    KL divergence KL(p || q) with safeguards.\n","    \"\"\"\n","    return entropy(p, qk=q)  # scipy.stats.entropy\n","\n","def js_divergence(p, q):\n","    \"\"\"\n","    JS divergence between p and q.\n","    scipy.spatial.distance.jensenshannon returns sqrt(JS), so we square it.\n","    \"\"\"\n","    return jensenshannon(p, q, base=2)**2\n","\n","# ------------------\n","# Quality metrics\n","# ------------------\n","\n","def compute_D(real_df, syn_df, bins=20):\n","    \"\"\"\n","    D: data distribution similarity.\n","    Uses per-column KS, KL, JSD + Range Coverage (RC),\n","    aggregated into a [0,1] similarity per column and then averaged.\n","    \"\"\"\n","    scores = []\n","    for col in real_df.columns:\n","        r = real_df[col].dropna().values\n","        s = syn_df[col].dropna().values\n","\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        # KS: 1 - KS statistic\n","        ks_stat, _ = ks_2samp(r, s)\n","        ks_sim = 1.0 - ks_stat  # in [0,1] if ks_stat <= 1\n","\n","        # Hist-based divergences\n","        p, q = hist_match(r, s, bins=bins)\n","\n","        # KL (two directions, average) → similarity\n","        kl_pq = safe_entropy(p, q)\n","        kl_qp = safe_entropy(q, p)\n","        kl_sym = 0.5 * (kl_pq + kl_qp)\n","        kl_sim = 1.0 / (1.0 + kl_sym)   # [0,1], larger KL → smaller similarity\n","\n","        # JS divergence → similarity\n","        jsd = js_divergence(p, q)       # [0,1]\n","        js_sim = 1.0 - jsd              # [0,1]\n","\n","        # Range coverage (RC): fraction of synthetic values within real min–max\n","        r_min, r_max = np.min(r), np.max(r)\n","        in_range = np.logical_and(s >= r_min, s <= r_max).mean()\n","        rc = float(in_range)            # already in [0,1]\n","\n","        col_score = np.mean([ks_sim, kl_sim, js_sim, rc])\n","        scores.append(col_score)\n","\n","    if not scores:\n","        return 0.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","def compute_A(real_df, syn_df):\n","    \"\"\"\n","    A: anomaly & rare-event pattern preservation using IQR.\n","    For each column:\n","      - define anomaly via Tukey IQR rule on real data\n","      - compare anomaly rates in real vs synthetic\n","      - score_j = 1 - |rate_real - rate_syn|\n","    Then average over columns.\n","    \"\"\"\n","    scores = []\n","    n_real = len(real_df)\n","    n_syn  = len(syn_df)\n","    for col in real_df.columns:\n","        r = real_df[col].dropna().values\n","        s = syn_df[col].dropna().values\n","\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        q1, q3 = np.percentile(r, [25, 75])\n","        iqr = q3 - q1\n","        if iqr == 0:\n","            continue\n","        lo = q1 - 1.5 * iqr\n","        hi = q3 + 1.5 * iqr\n","\n","        rare_r = np.logical_or(r < lo, r > hi).mean()\n","        rare_s = np.logical_or(s < lo, s > hi).mean()\n","\n","        diff = abs(rare_r - rare_s)\n","        score = 1.0 - diff  # if rates match exactly → 1.0\n","        scores.append(np.clip(score, 0.0, 1.0))\n","    if not scores:\n","        return 0.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","def compute_C(real_df, syn_df):\n","    \"\"\"\n","    C: association preservation via correlation similarity.\n","    Compute Pearson correlation matrices for real and synthetic,\n","    then 1 - mean absolute difference across (i,j) pairs.\n","    \"\"\"\n","    if real_df.shape[1] < 2:\n","        return 0.0\n","\n","    # Use numpy correlation (columns = variables)\n","    corr_real = np.corrcoef(real_df.values, rowvar=False)\n","    corr_syn  = np.corrcoef(syn_df.values,  rowvar=False)\n","\n","    # Ignore NaNs from constant columns\n","    mask = np.isfinite(corr_real) & np.isfinite(corr_syn)\n","    if not mask.any():\n","        return 0.0\n","\n","    diff = np.abs(corr_real - corr_syn)[mask]\n","    mean_diff = diff.mean()   # max possible difference per entry is 2.0\n","\n","    # Normalize: if mean_diff = 0 → 1, if mean_diff >= 2 → 0\n","    score = 1.0 - (mean_diff / 2.0)\n","    return float(np.clip(score, 0.0, 1.0))\n","\n","def compute_V(real_df, syn_df, bins=10):\n","    \"\"\"\n","    V: diversity / support coverage for numeric columns.\n","    For each column:\n","      - build histogram on real\n","      - consider only bins that are non-empty in real\n","      - measure fraction of those bins that are also non-empty in synthetic\n","    Average across columns.\n","    \"\"\"\n","    scores = []\n","    for col in real_df.columns:\n","        r = real_df[col].dropna().values\n","        s = syn_df[col].dropna().values\n","\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        r_min, r_max = np.min(r), np.max(r)\n","        if r_min == r_max:\n","            continue\n","\n","        hist_r, bin_edges = np.histogram(r, bins=bins, range=(r_min, r_max))\n","        hist_s, _        = np.histogram(s, bins=bin_edges)\n","\n","        real_support = hist_r > 0\n","        if not real_support.any():\n","            continue\n","\n","        syn_support  = hist_s > 0\n","        overlap = np.logical_and(real_support, syn_support).sum()\n","        coverage = overlap / real_support.sum()\n","        scores.append(float(np.clip(coverage, 0.0, 1.0)))\n","    if not scores:\n","        return 0.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","def compute_T(real_df, syn_df):\n","    \"\"\"\n","    T: temporal consistency placeholder.\n","    For purely static tabular data, this is set to 0.\n","    (For real temporal data, you would add DTW/auto-correlation-based scores.)\n","    \"\"\"\n","    return 0.0\n","\n","def compute_quality(real_df, syn_df, w_quality=WQ):\n","    \"\"\"\n","    Compute D, A, C, V, T and aggregate into Q.\n","    All sub-metrics are in [0,1].\n","    \"\"\"\n","    D = compute_D(real_df, syn_df)\n","    A = compute_A(real_df, syn_df)\n","    C = compute_C(real_df, syn_df)\n","    V = compute_V(real_df, syn_df)\n","    T = compute_T(real_df, syn_df)\n","\n","    Q = (\n","        w_quality[\"D\"] * D +\n","        w_quality[\"A\"] * A +\n","        w_quality[\"C\"] * C +\n","        w_quality[\"V\"] * V +\n","        w_quality[\"T\"] * T\n","    )\n","\n","    return {\n","        \"Q\": float(Q),\n","        \"D\": float(D),\n","        \"A\": float(A),\n","        \"C\": float(C),\n","        \"V\": float(V),\n","        \"T\": float(T),\n","    }\n","\n","# ------------------\n","# Privacy metrics\n","# ------------------\n","\n","def compute_DCR(real_df, syn_df):\n","    \"\"\"\n","    DCR: Distance to Closest Record.\n","    For each synthetic record, compute distance to closest real record.\n","    Return mean, std, and a normalized [0,1] version where larger is better.\n","    \"\"\"\n","    R = real_df.values.astype(float)\n","    S = syn_df.values.astype(float)\n","\n","    # Compute pairwise distances in batches to be safer on memory\n","    dists_min = []\n","    batch_size = 512\n","    for start in range(0, len(S), batch_size):\n","        end = min(len(S), start + batch_size)\n","        S_batch = S[start:end]\n","        # Euclidean distance to all real points\n","        diff = S_batch[:, None, :] - R[None, :, :]\n","        dist = np.linalg.norm(diff, axis=-1)\n","        d_min = dist.min(axis=1)\n","        dists_min.append(d_min)\n","    dists_min = np.concatenate(dists_min, axis=0)\n","\n","    mean_d = float(dists_min.mean())\n","    std_d  = float(dists_min.std())\n","    # Aggregate mean+std as overall DCR magnitude\n","    dcr_raw = mean_d + std_d\n","\n","    # Normalize with tanh to [0,1). Larger distance → closer to 1.\n","    dcr_norm = float(np.tanh(dcr_raw))\n","\n","    return {\n","        \"DCR_mean\": mean_d,\n","        \"DCR_std\": std_d,\n","        \"DCR_raw\": dcr_raw,\n","        \"DCR_norm\": dcr_norm,\n","    }\n","\n","def compute_Qdelta(real_df, syn_df, qs=(0.1, 0.25, 0.5, 0.75, 0.9)):\n","    \"\"\"\n","    Qδ: Quantile Difference statistic over numeric columns.\n","    For each column:\n","      - compute quantiles in real and synthetic at positions qs\n","      - take mean absolute difference\n","    Then average over columns.\n","    Normalize as qdelta_norm = qdelta_raw / (1 + qdelta_raw) ∈ [0,1).\n","    \"\"\"\n","    diffs = []\n","    qs = np.array(qs)\n","    for col in real_df.columns:\n","        r = real_df[col].dropna().values\n","        s = syn_df[col].dropna().values\n","\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        qr = np.quantile(r, qs)\n","        qs_syn = np.quantile(s, qs)\n","        diff = np.abs(qr - qs_syn).mean()\n","        diffs.append(diff)\n","\n","    if not diffs:\n","        qdelta_raw = 0.0\n","    else:\n","        qdelta_raw = float(np.mean(diffs))\n","\n","    qdelta_norm = float(qdelta_raw / (1.0 + qdelta_raw))  # (0,1)\n","    return {\"Qdelta_raw\": qdelta_raw, \"Qdelta_norm\": qdelta_norm}\n","\n","def compute_duplicates(real_df, syn_df, round_decimals=3):\n","    \"\"\"\n","    I: Duplicate analysis.\n","    - Within synthetic: fraction of duplicate rows\n","    - Between synthetic and real: fraction of synthetic rows that exactly\n","      match a real row.\n","    All comparisons are done after rounding to 'round_decimals' to make\n","    exact matching meaningful in floating point.\n","    \"\"\"\n","    def df_to_tuple(df):\n","        arr = np.round(df.values.astype(float), round_decimals)\n","        return [tuple(row) for row in arr]\n","\n","    syn_tuples  = df_to_tuple(syn_df)\n","    real_tuples = df_to_tuple(real_df)\n","\n","    n_syn = len(syn_tuples)\n","\n","    # Within synthetic duplicates\n","    from collections import Counter\n","    counts = Counter(syn_tuples)\n","    dup_within = sum(c for c in counts.values() if c > 1)\n","    dup_within_rate = dup_within / n_syn\n","\n","    # Cross duplicates\n","    real_set = set(real_tuples)\n","    dup_cross = sum(1 for t in syn_tuples if t in real_set)\n","    dup_cross_rate = dup_cross / n_syn\n","\n","    # Combine (simple average). This is already in [0,1].\n","    I_raw = float((dup_within_rate + dup_cross_rate) / 2.0)\n","    I_norm = float(np.clip(I_raw, 0.0, 1.0))\n","\n","    return {\n","        \"dup_within_rate\": float(dup_within_rate),\n","        \"dup_cross_rate\": float(dup_cross_rate),\n","        \"I_raw\": I_raw,\n","        \"I_norm\": I_norm,\n","    }\n","\n","def compute_privacy(real_df, syn_df, w_privacy=WP):\n","    \"\"\"\n","    Compute DCR, Qδ, I and aggregate into P:\n","        P = w1*normalize(DCR) - w2*normalize(Qδ) - w3*normalize(I)\n","    All normalized metrics are in [0,1].\n","    \"\"\"\n","    dcr = compute_DCR(real_df, syn_df)\n","    qd  = compute_Qdelta(real_df, syn_df)\n","    dup = compute_duplicates(real_df, syn_df)\n","\n","    DCR_norm = dcr[\"DCR_norm\"]\n","    Qdelta_norm = qd[\"Qdelta_norm\"]\n","    I_norm = dup[\"I_norm\"]\n","\n","    P = (\n","        w_privacy[\"DCR\"]   * DCR_norm -\n","        w_privacy[\"Qdelta\"] * Qdelta_norm -\n","        w_privacy[\"I\"]      * I_norm\n","    )\n","\n","    out = {}\n","    out.update(dcr)\n","    out.update(qd)\n","    out.update(dup)\n","    out[\"P\"] = float(P)\n","    return out\n","\n","# ------------------\n","# Main evaluation\n","# ------------------\n","\n","real_df = pd.read_csv(REAL_PATH)\n","syn_df  = pd.read_csv(SYN_PATH)\n","\n","# Numeric-only assumption (same as training cell)\n","for c in real_df.columns:\n","    if not pd.api.types.is_numeric_dtype(real_df[c]):\n","        raise ValueError(f\"Non-numeric column in real data: {c}\")\n","for c in syn_df.columns:\n","    if not pd.api.types.is_numeric_dtype(syn_df[c]):\n","        raise ValueError(f\"Non-numeric column in synthetic data: {c}\")\n","\n","real_df, syn_df = align_real_synth(real_df, syn_df)\n","\n","quality_metrics = compute_quality(real_df, syn_df, WQ)\n","privacy_metrics = compute_privacy(real_df, syn_df, WP)\n","\n","print(\"=== QUALITY METRICS (Q) ===\")\n","for k, v in quality_metrics.items():\n","    print(f\"{k:>3}: {v:.4f}\")\n","\n","print(\"\\n=== PRIVACY METRICS (P) ===\")\n","for k, v in privacy_metrics.items():\n","    if isinstance(v, float):\n","        print(f\"{k:>15}: {v:.6f}\")\n","    else:\n","        print(f\"{k:>15}: {v}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"_KidhvZnEBsA","executionInfo":{"status":"error","timestamp":1765228797153,"user_tz":480,"elapsed":5327,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"067c47ed-21b1-40e7-fba4-4258bc2a3984"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Non-numeric column in synthetic data: dataset","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1586269435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_numeric_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Non-numeric column in synthetic data: {c}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0mreal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malign_real_synth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Non-numeric column in synthetic data: dataset"]}]},{"cell_type":"code","source":["# ============================================\n","# Tabular synthetic data metrics (Quality + Privacy)\n","# ============================================\n","# Metrics:\n","#   Q_overall\n","#   Q_num(1-KS)\n","#   Q_cat(1-TVD)\n","#   Q_range\n","#   Q_anomaly\n","#   Q_corr\n","#   Q_diversity\n","#   P_overall\n","#   P_DCR\n","#   P_Qdelta\n","#   P_noDup\n","#   n_real, n_syn, num_cols_used, cat_cols_used\n","#\n","# Works for numeric + categorical columns.\n","# Categorical detection:\n","#   - auto: dtype == object/category\n","#   - or you can pass cat_cols=['col1', 'col2', ...]\n","# ============================================\n","\n","!pip install -q scipy\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import ks_2samp\n","from collections import Counter\n","\n","\n","# -------------------------\n","# Helpers: types & alignment\n","# -------------------------\n","\n","def _detect_column_types(df, cat_cols=None):\n","    cols = df.columns.tolist()\n","    if cat_cols is None:\n","        # auto-detect categoricals\n","        cat_cols = [c for c in cols\n","                    if df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"category\")]\n","    else:\n","        cat_cols = [c for c in cat_cols if c in cols]\n","\n","    num_cols = [c for c in cols if c not in cat_cols]\n","    return num_cols, cat_cols\n","\n","\n","def _align_real_synth(real_df, syn_df):\n","    # keep common columns, same order as real\n","    common = [c for c in real_df.columns if c in syn_df.columns]\n","    real_df = real_df[common].copy()\n","    syn_df  = syn_df[common].copy()\n","    # same number of rows (truncate to min)\n","    n = min(len(real_df), len(syn_df))\n","    real_df = real_df.iloc[:n].reset_index(drop=True)\n","    syn_df  = syn_df.iloc[:n].reset_index(drop=True)\n","    return real_df, syn_df\n","\n","\n","# -------------------------\n","# Quality metrics\n","# -------------------------\n","\n","def _q_num_1_minus_ks(real_df, syn_df, num_cols):\n","    \"\"\"Numeric quality: mean(1 - KS-statistic) across numeric columns.\"\"\"\n","    if not num_cols:\n","        return 1.0, 0\n","\n","    scores = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 2 or len(s) < 2:\n","            continue\n","        stat, _ = ks_2samp(r, s)\n","        scores.append(1.0 - stat)\n","\n","    if not scores:\n","        return 1.0, 0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0)), len(scores)\n","\n","\n","def _q_cat_1_minus_tvd(real_df, syn_df, cat_cols):\n","    \"\"\"Categorical quality: mean(1 - TVD) over categorical columns.\"\"\"\n","    if not cat_cols:\n","        return 1.0, 0\n","\n","    scores = []\n","    for c in cat_cols:\n","        r = real_df[c].astype(\"object\").fillna(\"nan\").values\n","        s = syn_df[c].astype(\"object\").fillna(\"nan\").values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","\n","        vals = sorted(set(r) | set(s))\n","        cr = Counter(r)\n","        cs = Counter(s)\n","        pr = np.array([cr[v] for v in vals], dtype=float)\n","        ps = np.array([cs[v] for v in vals], dtype=float)\n","        pr /= pr.sum()\n","        ps /= ps.sum()\n","        tvd = 0.5 * np.abs(pr - ps).sum()\n","        scores.append(1.0 - tvd)\n","\n","    if not scores:\n","        return 1.0, 0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0)), len(scores)\n","\n","\n","def _q_range(real_df, syn_df, num_cols):\n","    \"\"\"Q_range: fraction of synthetic values inside real min–max, averaged across numeric columns.\"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    covs = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","        lo, hi = np.min(r), np.max(r)\n","        if hi == lo:\n","            continue\n","        in_range = np.logical_and(s >= lo, s <= hi).mean()\n","        covs.append(in_range)\n","\n","    if not covs:\n","        return 1.0\n","    return float(np.clip(np.mean(covs), 0.0, 1.0))\n","\n","\n","def _q_anomaly(real_df, syn_df, num_cols):\n","    \"\"\"\n","    Q_anomaly: anomaly-rate preservation using IQR rule.\n","    For each numeric column:\n","      - define anomalies on real via Tukey rule\n","      - compare anomaly rates (real vs synthetic)\n","      - score_j = 1 - |rate_real - rate_syn|\n","    \"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    scores = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        q1, q3 = np.percentile(r, [25, 75])\n","        iqr = q3 - q1\n","        if iqr == 0:\n","            continue\n","\n","        lo = q1 - 1.5 * iqr\n","        hi = q3 + 1.5 * iqr\n","        rare_r = np.logical_or(r < lo, r > hi).mean()\n","        rare_s = np.logical_or(s < lo, s > hi).mean()\n","        score = 1.0 - abs(rare_r - rare_s)\n","        scores.append(score)\n","\n","    if not scores:\n","        return 1.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","\n","def _q_corr(real_df, syn_df, num_cols):\n","    \"\"\"\n","    Q_corr: correlation structure similarity across numeric features.\n","    1 - mean |corr_real - corr_syn| / 2.\n","    \"\"\"\n","    if len(num_cols) < 2:\n","        return 1.0\n","\n","    r_num = real_df[num_cols].astype(float).values\n","    s_num = syn_df[num_cols].astype(float).values\n","\n","    cr = np.corrcoef(r_num, rowvar=False)\n","    cs = np.corrcoef(s_num, rowvar=False)\n","\n","    mask = np.isfinite(cr) & np.isfinite(cs)\n","    if not mask.any():\n","        return 1.0\n","\n","    diff = np.abs(cr - cs)[mask]\n","    mean_diff = diff.mean()     # max diff per entry ~2\n","    score = 1.0 - (mean_diff / 2.0)\n","    return float(np.clip(score, 0.0, 1.0))\n","\n","\n","def _q_diversity(real_df, syn_df, num_cols, cat_cols, num_bins=10):\n","    \"\"\"\n","    Q_diversity:\n","      - numeric: support coverage via hist bins\n","      - categorical: category coverage\n","    \"\"\"\n","    scores = []\n","\n","    # numeric\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","        lo, hi = np.min(r), np.max(r)\n","        if hi == lo:\n","            continue\n","\n","        hr, edges = np.histogram(r, bins=num_bins, range=(lo, hi))\n","        hs, _ = np.histogram(s, bins=edges)\n","        real_support = hr > 0\n","        if not real_support.any():\n","            continue\n","        syn_support = hs > 0\n","        overlap = np.logical_and(real_support, syn_support).sum()\n","        cov = overlap / real_support.sum()\n","        scores.append(cov)\n","\n","    # categorical\n","    for c in cat_cols:\n","        r = real_df[c].astype(\"object\").fillna(\"nan\").values\n","        s = syn_df[c].astype(\"object\").fillna(\"nan\").values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","        cats_r = set(r)\n","        cats_s = set(s)\n","        if not cats_r:\n","            continue\n","        cov = len(cats_r & cats_s) / len(cats_r)\n","        scores.append(cov)\n","\n","    if not scores:\n","        return 1.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","\n","# -------------------------\n","# Privacy metrics\n","# -------------------------\n","\n","def _p_dcr(real_df, syn_df, num_cols, cat_cols):\n","    \"\"\"\n","    P_DCR: Distance to Closest Record (higher = more private).\n","    We:\n","      - map categoricals to integer codes\n","      - z-score all columns using real\n","      - compute mean min-distance(syn row, real dataset)\n","      - squash with tanh to [0,1)\n","    \"\"\"\n","    df_real = real_df.copy()\n","    df_syn  = syn_df.copy()\n","\n","    # encode categoricals\n","    for c in cat_cols:\n","        cats = sorted(\n","            set(df_real[c].dropna().astype(\"object\").unique()) |\n","            set(df_syn[c].dropna().astype(\"object\").unique())\n","        )\n","        mapping = {v: i for i, v in enumerate(cats)}\n","        df_real[c] = df_real[c].astype(\"object\").map(mapping).fillna(-1).astype(float)\n","        df_syn[c]  = df_syn[c].astype(\"object\").map(mapping).fillna(-1).astype(float)\n","\n","    arr_r = df_real.values.astype(float)\n","    arr_s = df_syn.values.astype(float)\n","\n","    means = np.nanmean(arr_r, axis=0, keepdims=True)\n","    stds = np.nanstd(arr_r, axis=0, keepdims=True)\n","    stds[stds == 0] = 1.0\n","    arr_r = (arr_r - means) / stds\n","    arr_s = (arr_s - means) / stds\n","\n","    d_min_all = []\n","    batch = 256\n","    for start in range(0, len(arr_s), batch):\n","        s_batch = arr_s[start:start+batch]\n","        diff = s_batch[:, None, :] - arr_r[None, :, :]\n","        dists = np.linalg.norm(diff, axis=-1)\n","        d_min = dists.min(axis=1)\n","        d_min_all.append(d_min)\n","\n","    d_min_all = np.concatenate(d_min_all, axis=0)\n","    mean_d = float(d_min_all.mean())\n","\n","    P_DCR = float(np.tanh(mean_d))   # in (0,1)\n","    return P_DCR\n","\n","\n","def _p_qdelta(real_df, syn_df, num_cols, quantiles=(0.1, 0.25, 0.5, 0.75, 0.9)):\n","    \"\"\"\n","    P_Qdelta: privacy from quantile differences on numeric columns.\n","    Smaller quantile mismatch => larger P_Qdelta.\n","    \"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    diffs = []\n","    qs = np.array(quantiles)\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","        qr = np.quantile(r, qs)\n","        qs_syn = np.quantile(s, qs)\n","        diffs.append(np.abs(qr - qs_syn).mean())\n","\n","    if not diffs:\n","        return 1.0\n","\n","    qdelta_raw = float(np.mean(diffs))\n","    P_Qdelta = 1.0 / (1.0 + qdelta_raw)  # (0,1]\n","    return float(np.clip(P_Qdelta, 0.0, 1.0))\n","\n","\n","def _p_no_dup(real_df, syn_df, round_decimals=3):\n","    \"\"\"\n","    P_noDup: 1 - duplicate rate\n","      - within synthetic\n","      - and synthetic rows that exactly match a real row\n","    \"\"\"\n","    def df_to_tuples(df):\n","        vals = df.copy()\n","        for c in vals.columns:\n","            if vals[c].dtype == \"object\":\n","                vals[c] = vals[c].fillna(\"nan\")\n","            else:\n","                vals[c] = np.round(vals[c].astype(float), round_decimals)\n","        return [tuple(row) for row in vals.values]\n","\n","    syn_t  = df_to_tuples(syn_df)\n","    real_t = set(df_to_tuples(real_df))\n","    n_syn = len(syn_t)\n","\n","    from collections import Counter\n","    counts = Counter(syn_t)\n","\n","    dup_within = sum(c for c in counts.values() if c > 1)\n","    dup_within_rate = dup_within / n_syn\n","\n","    dup_cross = sum(1 for t in syn_t if t in real_t)\n","    dup_cross_rate = dup_cross / n_syn\n","\n","    dup_rate = (dup_within_rate + dup_cross_rate) / 2.0\n","    P_noDup = 1.0 - dup_rate\n","    return float(np.clip(P_noDup, 0.0, 1.0))\n","\n","\n","# -------------------------\n","# Main entry\n","# -------------------------\n","\n","def evaluate_pair(\n","    real_path: str,\n","    syn_path: str,\n","    dataset_name: str = None,\n","    synthetic_name: str = None,\n","    cat_cols=None,  # optional list of categorical columns\n","):\n","    \"\"\"\n","    Compute all metrics for one (real, synthetic) pair.\n","\n","    Returns a 1-row pandas.DataFrame with columns:\n","      dataset, synthetic,\n","      Q_overall, Q_num(1-KS), Q_cat(1-TVD), Q_range, Q_anomaly, Q_corr, Q_diversity,\n","      P_overall, P_DCR, P_Qdelta, P_noDup,\n","      n_real, n_syn, num_cols_used, cat_cols_used\n","    \"\"\"\n","    real_df = pd.read_csv(real_path)\n","    syn_df  = pd.read_csv(syn_path)\n","\n","    real_df, syn_df = _align_real_synth(real_df, syn_df)\n","\n","    num_cols, cat_cols_detected = _detect_column_types(real_df, cat_cols)\n","    cat_cols_used = cat_cols_detected\n","    num_cols_used = num_cols\n","\n","    # Quality\n","    Q_num, _ = _q_num_1_minus_ks(real_df, syn_df, num_cols_used)\n","    Q_cat, _ = _q_cat_1_minus_tvd(real_df, syn_df, cat_cols_used)\n","    Q_range = _q_range(real_df, syn_df, num_cols_used)\n","    Q_anomaly = _q_anomaly(real_df, syn_df, num_cols_used)\n","    Q_corr = _q_corr(real_df, syn_df, num_cols_used)\n","    Q_diversity = _q_diversity(real_df, syn_df, num_cols_used, cat_cols_used)\n","\n","    Q_components = [Q_num, Q_cat, Q_range, Q_anomaly, Q_corr, Q_diversity]\n","    Q_overall = float(np.mean(Q_components))\n","\n","    # Privacy\n","    P_DCR = _p_dcr(real_df, syn_df, num_cols_used, cat_cols_used)\n","    P_Qdelta = _p_qdelta(real_df, syn_df, num_cols_used)\n","    P_noDup = _p_no_dup(real_df, syn_df)\n","\n","    P_overall = float(np.mean([P_DCR, P_Qdelta, P_noDup]))\n","\n","    result = {\n","        \"dataset\": dataset_name if dataset_name is not None else real_path,\n","        \"synthetic\": synthetic_name if synthetic_name is not None else syn_path,\n","        \"Q_overall\": Q_overall,\n","        \"Q_num(1-KS)\": Q_num,\n","        \"Q_cat(1-TVD)\": Q_cat,\n","        \"Q_range\": Q_range,\n","        \"Q_anomaly\": Q_anomaly,\n","        \"Q_corr\": Q_corr,\n","        \"Q_diversity\": Q_diversity,\n","        \"P_overall\": P_overall,\n","        \"P_DCR\": P_DCR,\n","        \"P_Qdelta\": P_Qdelta,\n","        \"P_noDup\": P_noDup,\n","        \"n_real\": len(real_df),\n","        \"n_syn\": len(syn_df),\n","        \"num_cols_used\": len(num_cols_used),\n","        \"cat_cols_used\": len(cat_cols_used),\n","    }\n","    return pd.DataFrame([result])\n","\n","\n","# -------------------------\n","# Example usage in Colab\n","# -------------------------\n","# metrics = evaluate_pair(\n","#     real_path=\"breast-cancer-wisconsin.csv\",\n","#     syn_path=\"breast-cancer-wisconsin_CTABGAN_DP.csv\",\n","#     dataset_name=\"breast-cancer-wisconsin.csv\",\n","#     synthetic_name=\"breast-cancer-wisconsin_CTABGAN_DP.csv\",\n","#     cat_cols=None,  # or ['some_categorical_col']\n","# )\n","# print(metrics)\n"],"metadata":{"id":"P0vxDuzgLPbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================\n","# Tabular synthetic data metrics (Quality + Privacy)\n","# ============================================\n","# Metrics:\n","#   Q_overall\n","#   Q_num(1-KS)\n","#   Q_cat(1-TVD)\n","#   Q_range\n","#   Q_anomaly\n","#   Q_corr\n","#   Q_diversity\n","#   P_overall\n","#   P_DCR\n","#   P_Qdelta\n","#   P_noDup\n","#   n_real, n_syn, num_cols_used, cat_cols_used\n","# ============================================\n","\n","!pip install -q scipy\n","\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import ks_2samp\n","from collections import Counter\n","\n","\n","# -------------------------\n","# Helpers: types & alignment\n","# -------------------------\n","\n","def _detect_column_types(df, cat_cols=None):\n","    cols = df.columns.tolist()\n","    if cat_cols is None:\n","        # auto-detect categoricals\n","        cat_cols = [c for c in cols\n","                    if df[c].dtype == \"object\" or str(df[c].dtype).startswith(\"category\")]\n","    else:\n","        cat_cols = [c for c in cat_cols if c in cols]\n","\n","    num_cols = [c for c in cols if c not in cat_cols]\n","    return num_cols, cat_cols\n","\n","\n","def _align_real_synth(real_df, syn_df):\n","    # keep common columns, same order as real\n","    common = [c for c in real_df.columns if c in syn_df.columns]\n","    real_df = real_df[common].copy()\n","    syn_df  = syn_df[common].copy()\n","    # same number of rows (truncate to min)\n","    n = min(len(real_df), len(syn_df))\n","    real_df = real_df.iloc[:n].reset_index(drop=True)\n","    syn_df  = syn_df.iloc[:n].reset_index(drop=True)\n","    return real_df, syn_df\n","\n","\n","# -------------------------\n","# Quality metrics\n","# -------------------------\n","\n","def _q_num_1_minus_ks(real_df, syn_df, num_cols):\n","    \"\"\"Numeric quality: mean(1 - KS-statistic) across numeric columns.\"\"\"\n","    if not num_cols:\n","        return 1.0, 0\n","\n","    scores = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 2 or len(s) < 2:\n","            continue\n","        stat, _ = ks_2samp(r, s)\n","        scores.append(1.0 - stat)\n","\n","    if not scores:\n","        return 1.0, 0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0)), len(scores)\n","\n","\n","def _q_cat_1_minus_tvd(real_df, syn_df, cat_cols):\n","    \"\"\"Categorical quality: mean(1 - TVD) over categorical columns.\"\"\"\n","    if not cat_cols:\n","        return 1.0, 0\n","\n","    scores = []\n","    for c in cat_cols:\n","        r = real_df[c].astype(\"object\").fillna(\"nan\").values\n","        s = syn_df[c].astype(\"object\").fillna(\"nan\").values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","\n","        vals = sorted(set(r) | set(s))\n","        cr = Counter(r)\n","        cs = Counter(s)\n","        pr = np.array([cr[v] for v in vals], dtype=float)\n","        ps = np.array([cs[v] for v in vals], dtype=float)\n","        pr /= pr.sum()\n","        ps /= ps.sum()\n","        tvd = 0.5 * np.abs(pr - ps).sum()\n","        scores.append(1.0 - tvd)\n","\n","    if not scores:\n","        return 1.0, 0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0)), len(scores)\n","\n","\n","def _q_range(real_df, syn_df, num_cols):\n","    \"\"\"Q_range: fraction of synthetic values inside real min–max, averaged across numeric columns.\"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    covs = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","        lo, hi = np.min(r), np.max(r)\n","        if hi == lo:\n","            continue\n","        in_range = np.logical_and(s >= lo, s <= hi).mean()\n","        covs.append(in_range)\n","\n","    if not covs:\n","        return 1.0\n","    return float(np.clip(np.mean(covs), 0.0, 1.0))\n","\n","\n","def _q_anomaly(real_df, syn_df, num_cols):\n","    \"\"\"\n","    Q_anomaly: anomaly-rate preservation using IQR rule.\n","    For each numeric column:\n","      - define anomalies on real via Tukey rule\n","      - compare anomaly rates (real vs synthetic)\n","      - score_j = 1 - |rate_real - rate_syn|\n","    \"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    scores = []\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","\n","        q1, q3 = np.percentile(r, [25, 75])\n","        iqr = q3 - q1\n","        if iqr == 0:\n","            continue\n","\n","        lo = q1 - 1.5 * iqr\n","        hi = q3 + 1.5 * iqr\n","        rare_r = np.logical_or(r < lo, r > hi).mean()\n","        rare_s = np.logical_or(s < lo, s > hi).mean()\n","        score = 1.0 - abs(rare_r - rare_s)\n","        scores.append(score)\n","\n","    if not scores:\n","        return 1.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","\n","def _q_corr(real_df, syn_df, num_cols):\n","    \"\"\"\n","    Q_corr: correlation structure similarity across numeric features.\n","    1 - mean |corr_real - corr_syn| / 2.\n","    \"\"\"\n","    if len(num_cols) < 2:\n","        return 1.0\n","\n","    r_num = real_df[num_cols].astype(float).values\n","    s_num = syn_df[num_cols].astype(float).values\n","\n","    cr = np.corrcoef(r_num, rowvar=False)\n","    cs = np.corrcoef(s_num, rowvar=False)\n","\n","    mask = np.isfinite(cr) & np.isfinite(cs)\n","    if not mask.any():\n","        return 1.0\n","\n","    diff = np.abs(cr - cs)[mask]\n","    mean_diff = diff.mean()     # max diff per entry ~2\n","    score = 1.0 - (mean_diff / 2.0)\n","    return float(np.clip(score, 0.0, 1.0))\n","\n","\n","def _q_diversity(real_df, syn_df, num_cols, cat_cols, num_bins=10):\n","    \"\"\"\n","    Q_diversity:\n","      - numeric: support coverage via hist bins\n","      - categorical: category coverage\n","    \"\"\"\n","    scores = []\n","\n","    # numeric\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","        lo, hi = np.min(r), np.max(r)\n","        if hi == lo:\n","            continue\n","\n","        hr, edges = np.histogram(r, bins=num_bins, range=(lo, hi))\n","        hs, _ = np.histogram(s, bins=edges)\n","        real_support = hr > 0\n","        if not real_support.any():\n","            continue\n","        syn_support = hs > 0\n","        overlap = np.logical_and(real_support, syn_support).sum()\n","        cov = overlap / real_support.sum()\n","        scores.append(cov)\n","\n","    # categorical\n","    for c in cat_cols:\n","        r = real_df[c].astype(\"object\").fillna(\"nan\").values\n","        s = syn_df[c].astype(\"object\").fillna(\"nan\").values\n","        if len(r) == 0 or len(s) == 0:\n","            continue\n","        cats_r = set(r)\n","        cats_s = set(s)\n","        if not cats_r:\n","            continue\n","        cov = len(cats_r & cats_s) / len(cats_r)\n","        scores.append(cov)\n","\n","    if not scores:\n","        return 1.0\n","    return float(np.clip(np.mean(scores), 0.0, 1.0))\n","\n","\n","# -------------------------\n","# Privacy metrics\n","# -------------------------\n","\n","def _p_dcr(real_df, syn_df, num_cols, cat_cols):\n","    \"\"\"\n","    P_DCR: Distance to Closest Record (higher = more private).\n","    We:\n","      - map categoricals to integer codes\n","      - z-score all columns using real\n","      - compute mean min-distance(syn row, real dataset)\n","      - squash with tanh to [0,1)\n","    \"\"\"\n","    df_real = real_df.copy()\n","    df_syn  = syn_df.copy()\n","\n","    # encode categoricals\n","    for c in cat_cols:\n","        cats = sorted(\n","            set(df_real[c].dropna().astype(\"object\").unique()) |\n","            set(df_syn[c].dropna().astype(\"object\").unique())\n","        )\n","        mapping = {v: i for i, v in enumerate(cats)}\n","        df_real[c] = df_real[c].astype(\"object\").map(mapping).fillna(-1).astype(float)\n","        df_syn[c]  = df_syn[c].astype(\"object\").map(mapping).fillna(-1).astype(float)\n","\n","    arr_r = df_real.values.astype(float)\n","    arr_s = df_syn.values.astype(float)\n","\n","    means = np.nanmean(arr_r, axis=0, keepdims=True)\n","    stds = np.nanstd(arr_r, axis=0, keepdims=True)\n","    stds[stds == 0] = 1.0\n","    arr_r = (arr_r - means) / stds\n","    arr_s = (arr_s - means) / stds\n","\n","    d_min_all = []\n","    batch = 256\n","    for start in range(0, len(arr_s), batch):\n","        s_batch = arr_s[start:start+batch]\n","        diff = s_batch[:, None, :] - arr_r[None, :, :]\n","        dists = np.linalg.norm(diff, axis=-1)\n","        d_min = dists.min(axis=1)\n","        d_min_all.append(d_min)\n","\n","    d_min_all = np.concatenate(d_min_all, axis=0)\n","    mean_d = float(d_min_all.mean())\n","\n","    P_DCR = float(np.tanh(mean_d))   # in (0,1)\n","    return P_DCR\n","\n","\n","def _p_qdelta(real_df, syn_df, num_cols, quantiles=(0.1, 0.25, 0.5, 0.75, 0.9)):\n","    \"\"\"\n","    P_Qdelta: privacy from quantile differences on numeric columns.\n","    Smaller quantile mismatch => larger P_Qdelta.\n","    \"\"\"\n","    if not num_cols:\n","        return 1.0\n","\n","    diffs = []\n","    qs = np.array(quantiles)\n","    for c in num_cols:\n","        r = real_df[c].dropna().values\n","        s = syn_df[c].dropna().values\n","        if len(r) < 10 or len(s) < 10:\n","            continue\n","        qr = np.quantile(r, qs)\n","        qs_syn = np.quantile(s, qs)\n","        diffs.append(np.abs(qr - qs_syn).mean())\n","\n","    if not diffs:\n","        return 1.0\n","\n","    qdelta_raw = float(np.mean(diffs))\n","    P_Qdelta = 1.0 / (1.0 + qdelta_raw)  # (0,1]\n","    return float(np.clip(P_Qdelta, 0.0, 1.0))\n","\n","\n","def _p_no_dup(real_df, syn_df, round_decimals=3):\n","    \"\"\"\n","    P_noDup: 1 - duplicate rate\n","      - within synthetic\n","      - and synthetic rows that exactly match a real row\n","    \"\"\"\n","    def df_to_tuples(df):\n","        vals = df.copy()\n","        for c in vals.columns:\n","            if vals[c].dtype == \"object\":\n","                vals[c] = vals[c].fillna(\"nan\")\n","            else:\n","                vals[c] = np.round(vals[c].astype(float), round_decimals)\n","        return [tuple(row) for row in vals.values]\n","\n","    syn_t  = df_to_tuples(syn_df)\n","    real_t = set(df_to_tuples(real_df))\n","    n_syn = len(syn_t)\n","\n","    from collections import Counter\n","    counts = Counter(syn_t)\n","\n","    dup_within = sum(c for c in counts.values() if c > 1)\n","    dup_within_rate = dup_within / n_syn\n","\n","    dup_cross = sum(1 for t in syn_t if t in real_t)\n","    dup_cross_rate = dup_cross / n_syn\n","\n","    dup_rate = (dup_within_rate + dup_cross_rate) / 2.0\n","    P_noDup = 1.0 - dup_rate\n","    return float(np.clip(P_noDup, 0.0, 1.0))\n","\n","\n","# -------------------------\n","# Main entry\n","# -------------------------\n","\n","def evaluate_pair(\n","    real_path: str,\n","    syn_path: str,\n","    dataset_name: str = None,\n","    synthetic_name: str = None,\n","    cat_cols=None,  # optional list of categorical columns\n","):\n","    \"\"\"\n","    Compute all metrics for one (real, synthetic) pair.\n","\n","    Returns a 1-row pandas.DataFrame with columns:\n","      dataset, synthetic,\n","      Q_overall, Q_num(1-KS), Q_cat(1-TVD), Q_range, Q_anomaly, Q_corr, Q_diversity,\n","      P_overall, P_DCR, P_Qdelta, P_noDup,\n","      n_real, n_syn, num_cols_used, cat_cols_used\n","    \"\"\"\n","    real_df = pd.read_csv(real_path)\n","    syn_df  = pd.read_csv(syn_path)\n","\n","    real_df, syn_df = _align_real_synth(real_df, syn_df)\n","\n","    num_cols, cat_cols_detected = _detect_column_types(real_df, cat_cols)\n","    cat_cols_used = cat_cols_detected\n","    num_cols_used = num_cols\n","\n","    # Quality\n","    Q_num, _ = _q_num_1_minus_ks(real_df, syn_df, num_cols_used)\n","    Q_cat, _ = _q_cat_1_minus_tvd(real_df, syn_df, cat_cols_used)\n","    Q_range = _q_range(real_df, syn_df, num_cols_used)\n","    Q_anomaly = _q_anomaly(real_df, syn_df, num_cols_used)\n","    Q_corr = _q_corr(real_df, syn_df, num_cols_used)\n","    Q_diversity = _q_diversity(real_df, syn_df, num_cols_used, cat_cols_used)\n","\n","    Q_components = [Q_num, Q_cat, Q_range, Q_anomaly, Q_corr, Q_diversity]\n","    Q_overall = float(np.mean(Q_components))\n","\n","    # Privacy\n","    P_DCR = _p_dcr(real_df, syn_df, num_cols_used, cat_cols_used)\n","    P_Qdelta = _p_qdelta(real_df, syn_df, num_cols_used)\n","    P_noDup = _p_no_dup(real_df, syn_df)\n","\n","    P_overall = float(np.mean([P_DCR, P_Qdelta, P_noDup]))\n","\n","    result = {\n","        \"dataset\": dataset_name if dataset_name is not None else real_path,\n","        \"synthetic\": synthetic_name if synthetic_name is not None else syn_path,\n","        \"Q_overall\": Q_overall,\n","        \"Q_num(1-KS)\": Q_num,\n","        \"Q_cat(1-TVD)\": Q_cat,\n","        \"Q_range\": Q_range,\n","        \"Q_anomaly\": Q_anomaly,\n","        \"Q_corr\": Q_corr,\n","        \"Q_diversity\": Q_diversity,\n","        \"P_overall\": P_overall,\n","        \"P_DCR\": P_DCR,\n","        \"P_Qdelta\": P_Qdelta,\n","        \"P_noDup\": P_noDup,\n","        \"n_real\": len(real_df),\n","        \"n_syn\": len(syn_df),\n","        \"num_cols_used\": len(num_cols_used),\n","        \"cat_cols_used\": len(cat_cols_used),\n","    }\n","    return pd.DataFrame([result])\n","\n","\n","# -------------------------\n","# Interactive usage\n","# -------------------------\n","if __name__ == \"__main__\":\n","    # In Colab this will ask in the cell output area:\n","    real_path = input(\"Enter path to REAL data CSV (e.g. breast-cancer-wisconsin.csv): \").strip()\n","    syn_path  = input(\"Enter path to SYNTHETIC data CSV (e.g. breast-cancer-wisconsin_CTABGAN_DP.csv): \").strip()\n","\n","    metrics = evaluate_pair(\n","        real_path=real_path,\n","        syn_path=syn_path,\n","        dataset_name=real_path,\n","        synthetic_name=syn_path,\n","        cat_cols=None,  # or e.g. ['diagnosis', 'gender'] if you want to force categoricals\n","    )\n","\n","    # Wide (original) view – optional\n","    print(\"\\n==== Metrics (wide, 1 row) ====\\n\")\n","    print(metrics.to_string(index=False))\n","\n","    # Vertical view: key–value pairs\n","    print(\"\\n==== Metrics (vertical) ====\\n\")\n","    vertical = metrics.T.reset_index()\n","    vertical.columns = [\"Metric\", \"Value\"]\n","    print(vertical.to_string(index=False))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zo7cqZfqM809","executionInfo":{"status":"ok","timestamp":1765218842019,"user_tz":480,"elapsed":26377,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"fd99f238-be22-4135-f3cd-6707a8b7c374"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter path to REAL data CSV (e.g. breast-cancer-wisconsin.csv): /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\n","Enter path to SYNTHETIC data CSV (e.g. breast-cancer-wisconsin_CTABGAN_DP.csv): /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv\n","\n","==== Metrics (wide, 1 row) ====\n","\n","                                                                                      dataset                                                                                synthetic  Q_overall  Q_num(1-KS)  Q_cat(1-TVD)  Q_range  Q_anomaly   Q_corr  Q_diversity  P_overall    P_DCR  P_Qdelta  P_noDup  n_real  n_syn  num_cols_used  cat_cols_used\n","/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv   0.741425     0.408625           1.0 0.999601   0.953294 0.791574     0.295455   0.656152 0.968378  0.000077      1.0     683    683             11              0\n","\n","==== Metrics (vertical) ====\n","\n","       Metric                                                                                         Value\n","      dataset /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\n","    synthetic      /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv\n","    Q_overall                                                                                      0.741425\n","  Q_num(1-KS)                                                                                      0.408625\n"," Q_cat(1-TVD)                                                                                           1.0\n","      Q_range                                                                                      0.999601\n","    Q_anomaly                                                                                      0.953294\n","       Q_corr                                                                                      0.791574\n","  Q_diversity                                                                                      0.295455\n","    P_overall                                                                                      0.656152\n","        P_DCR                                                                                      0.968378\n","     P_Qdelta                                                                                      0.000077\n","      P_noDup                                                                                           1.0\n","       n_real                                                                                           683\n","        n_syn                                                                                           683\n","num_cols_used                                                                                            11\n","cat_cols_used                                                                                             0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n","from sklearn.linear_model import LogisticRegression\n","from copy import deepcopy\n","\n","# ---------------------------------------------------\n","# 1) Paths to REAL and SYNTHETIC data\n","# ---------------------------------------------------\n","real_path = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"\n","syn_path  = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/synthetic_data_alm.csv\"\n","\n","# Set your label column name here:\n","TARGET_COL = \"Class\"   # <-- change this if your target is called something else\n","\n","# ---------------------------------------------------\n","# 2) Load and align data\n","# ---------------------------------------------------\n","real_df = pd.read_csv(real_path)\n","syn_df  = pd.read_csv(syn_path)\n","\n","if TARGET_COL not in real_df.columns:\n","    raise ValueError(f\"TARGET_COL='{TARGET_COL}' not in REAL columns: {real_df.columns.tolist()}\")\n","if TARGET_COL not in syn_df.columns:\n","    raise ValueError(f\"TARGET_COL='{TARGET_COL}' not in SYN columns: {syn_df.columns.tolist()}\")\n","\n","# Keep only common columns in the same order\n","common_cols = [c for c in real_df.columns if c in syn_df.columns]\n","real_df = real_df[common_cols].copy()\n","syn_df  = syn_df[common_cols].copy()\n","\n","# ---------------------------------------------------\n","# 3) Split into features (X) and label (y)\n","# ---------------------------------------------------\n","X_real = real_df.drop(columns=[TARGET_COL])\n","y_real = real_df[TARGET_COL]\n","\n","X_syn = syn_df.drop(columns=[TARGET_COL])\n","y_syn = syn_df[TARGET_COL]\n","\n","# ---------------------------------------------------\n","# 4) Train/test split on REAL data (always test on REAL)\n","# ---------------------------------------------------\n","X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n","    X_real, y_real, test_size=0.3, random_state=42, stratify=y_real\n",")\n","\n","# ---------------------------------------------------\n","# 5) Preprocessing + model\n","# ---------------------------------------------------\n","numeric_features = X_real.select_dtypes(include=[\"int64\",\"float64\",\"int32\",\"float32\"]).columns.tolist()\n","categorical_features = [c for c in X_real.columns if c not in numeric_features]\n","\n","preprocess = ColumnTransformer(\n","    transformers=[\n","        (\"num\", StandardScaler(), numeric_features),\n","        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n","    ],\n","    remainder=\"drop\",\n",")\n","\n","clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n","\n","def fit_and_eval(X_train, y_train, X_test, y_test, desc=\"\"):\n","    pipe = Pipeline([\n","        (\"preprocess\", deepcopy(preprocess)),\n","        (\"clf\", deepcopy(clf)),\n","    ])\n","    pipe.fit(X_train, y_train)\n","    y_pred = pipe.predict(X_test)\n","\n","    classes = np.unique(y_train)\n","    binary = len(classes) == 2\n","\n","    acc = accuracy_score(y_test, y_pred)\n","    if binary:\n","        f1 = f1_score(y_test, y_pred, average=\"binary\", pos_label=classes[1])\n","    else:\n","        f1 = f1_score(y_test, y_pred, average=\"macro\")\n","\n","    try:\n","        y_proba = pipe.predict_proba(X_test)\n","        if binary:\n","            pos_idx = list(pipe.named_steps[\"clf\"].classes_).index(classes[1])\n","            y_score = y_proba[:, pos_idx]\n","            auc = roc_auc_score(y_test, y_score)\n","        else:\n","            auc = roc_auc_score(y_test, y_proba, multi_class=\"ovr\", average=\"macro\")\n","    except Exception:\n","        auc = np.nan\n","\n","    return {\n","        \"setting\": desc,\n","        \"ACC\": acc,\n","        \"F1\": f1,\n","        \"AUC\": auc,\n","    }\n","\n","# ---------------------------------------------------\n","# 6) Evaluate:\n","#     - Train on REAL, test on REAL (baseline)\n","#     - Train on SYN,  test on REAL (utility of synthetic)\n","# ---------------------------------------------------\n","metrics_real = fit_and_eval(\n","    X_train_real, y_train_real, X_test_real, y_test_real,\n","    desc=\"Train on REAL, test on REAL\"\n",")\n","metrics_syn  = fit_and_eval(\n","    X_syn, y_syn, X_test_real, y_test_real,\n","    desc=\"Train on SYN,  test on REAL\"\n",")\n","\n","results_df = pd.DataFrame([metrics_real, metrics_syn])\n","\n","# ---------------------------------------------------\n","# 7) Tabular output (metrics as rows, settings as columns)\n","# ---------------------------------------------------\n","# Reformat so rows = ACC/F1/AUC and columns = each setting\n","settings = results_df[\"setting\"].tolist()\n","metric_names = [\"ACC\", \"F1\", \"AUC\"]\n","\n","vertical_table = results_df.set_index(\"setting\")[metric_names].T.reset_index()\n","vertical_table.rename(columns={\"index\": \"Metric\"}, inplace=True)\n","\n","print(\"\\n==== Classification metrics (tabular) ====\\n\")\n","print(vertical_table.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"cVJ6Gqv_wb0S","executionInfo":{"status":"error","timestamp":1765228345839,"user_tz":480,"elapsed":1886,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"15717b52-9b36-4a9f-d496-a6325e6d8703"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3472851625.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train on REAL, test on REAL\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m )\n\u001b[0;32m--> 113\u001b[0;31m metrics_syn  = fit_and_eval(\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0mX_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_syn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Train on SYN,  test on REAL\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3472851625.py\u001b[0m in \u001b[0;36mfit_and_eval\u001b[0;34m(X_train, y_train, X_test, y_test, desc)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m\"clf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     ])\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 )\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1229\u001b[0m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"liblinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sag\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saga\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         )\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ]:\n\u001b[0;32m--> 222\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;34mf\"Unknown label type: {y_type}. Maybe you are trying to fit a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"classifier, which expects discrete classes on a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."]}]},{"cell_type":"code","source":["# save as build_poster_slide.py and run:  python build_poster_slide.py\n","from pptx import Presentation\n","from pptx.util import Inches\n","from PIL import Image, ImageDraw, ImageFont\n","\n","# 1) Draw the flowchart to a PNG (pure Pillow for portability)\n","W,H = 3200, 1400\n","img = Image.new(\"RGB\", (W,H), \"white\")\n","d = ImageDraw.Draw(img)\n","def rounded(x,y,w,h,r,fill): d.rounded_rectangle([x,y,x+w,y+h], r, fill=fill)\n","\n","# colors\n","title=\"#b6c23d\"; c1=\"#f0c808\"; c2=\"#d99a00\"; c3=\"#9bbf6a\"; c4=\"#b9b2ad\"; c5=\"#e38b42\"; foot=\"#e3d66e\"; txt=\"#111111\"\n","\n","# title\n","rounded(40,40,W-80,150,20,title); d.text((60,95),\"IDEAL: Interactive Detection & Explanation of Anomalies (LSTM Autoencoder)\", fill=txt, anchor=\"lm\")\n","\n","# boxes\n","boxes=[(c1,\"Data Preparation\",[\"Clean / encode\",\"Set window size\"]),\n","       (c2,\"Constraint Discovery\",[\"LSTM-AE learns\",\"Long-term constraints\"]),\n","       (c3,\"Anomaly Detection\",[\"Detect suspicious\",\"Records & sequences\"]),\n","       (c4,\"Anomaly Interpretation\",[\"Explain violations\",\"Feature importances\"]),\n","       (c5,\"Anomaly Inspection\",[\"Expert review\",\"Retrain with feedback\"])]\n","x0=120; y0=380; bw=520; bh=420; gap=80\n","for i,(c,title_txt,bul) in enumerate(boxes):\n","    x = x0 + i*(bw+gap)\n","    rounded(x,y0,bw,bh,28,c)\n","    d.text((x+bw/2,y0+60), title_txt, fill=txt, anchor=\"mm\")\n","    d.text((x+30,y0+140), f\"• {bul[0]}\\n• {bul[1]}\", fill=txt, anchor=\"la\")\n","\n","    # arrow to next\n","    if i<4:\n","        ax = x+bw+10; ay = y0+bh/2\n","        d.line([(ax,ay),(ax+gap-20,ay)], fill=\"#6b6b6b\", width=8)\n","        d.polygon([(ax+gap-20,ay-14),(ax+gap,ay),(ax+gap-20,ay+14)], fill=\"#6b6b6b\")\n","\n","# footer\n","rounded(40,H-170,W-80,130,20,foot)\n","d.text((W/2,H-105), \"Suspicious Sequence Detected from Johns Hopkins COVID-19 Data\", fill=txt, anchor=\"mm\")\n","\n","png_path = \"flowchart_ctabgan_dp.png\"\n","img.save(png_path, dpi=(300,300))\n","\n","# 2) Drop the PNG into a PPTX slide\n","prs = Presentation()\n","prs.slide_width = Inches(13.333)   # 16:9\n","prs.slide_height = Inches(7.5)\n","slide = prs.slides.add_slide(prs.slide_layouts[6])\n","slide.shapes.add_picture(png_path, Inches(0.25), Inches(0.25), width=prs.slide_width-Inches(0.5))\n","prs.save(\"CTABGAN_DP_Flowchart.pptx\")\n","print(\"Wrote CTABGAN_DP_Flowchart.pptx\")\n"],"metadata":{"id":"WcPm8Ql89T02","executionInfo":{"status":"error","timestamp":1765230859808,"user_tz":480,"elapsed":38,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"9936e008-cf99-4868-8cc1-fad0bc092b6f","colab":{"base_uri":"https://localhost:8080/","height":383}},"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pptx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1368758144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save as build_poster_slide.py and run:  python build_poster_slide.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpptx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPresentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpptx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pptx'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}