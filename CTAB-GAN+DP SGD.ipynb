{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","mount_file_id":"1FyG2wKs3lLCDr7Ks_uL35waJHgSptxTG","authorship_tag":"ABX9TyPlB6VGii+XT57RPts7ZFPF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9hUKaND89zU","executionInfo":{"status":"ok","timestamp":1765247952764,"user_tz":480,"elapsed":14193,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"8c0af9c5-5c72-4a36-8d89-6aa1b86e2e3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","# then, for convenience\n","%cd /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymUg0xa79q14","executionInfo":{"status":"ok","timestamp":1765219162831,"user_tz":480,"elapsed":208,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"e717e876-ea88-40c3-b090-98f2c2ae5ad3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Dec  8 18:39:21 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["%pip install --upgrade pip setuptools wheel\n","\n","# Let pip choose compatible versions for your Python:\n","%pip install opacus numpy pandas scikit-learn scipy dython\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6sQpnMv3Nse2","executionInfo":{"status":"ok","timestamp":1765219157995,"user_tz":480,"elapsed":17001,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"4532b203-db5d-4b30-f3a0-bd0cb0b786f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n","Collecting setuptools\n","  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n","Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 75.2.0\n","    Uninstalling setuptools-75.2.0:\n","      Successfully uninstalled setuptools-75.2.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack"]},"id":"18022d8a0a7b4e4d8515da7c86b88159"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting opacus\n","  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n","Collecting dython\n","  Downloading dython-0.7.11-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (2.9.0+cu126)\n","Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n","Collecting numpy\n","  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n","Collecting pandas\n","  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n","Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from dython) (0.13.2)\n","Collecting matplotlib>=3.10.6 (from dython)\n","  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting psutil>=7.1.0 (from dython)\n","  Downloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (4.61.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (11.3.0)\n","Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.10.6->dython) (3.2.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (80.9.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->opacus) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->opacus) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.3)\n","Downloading opacus-1.5.4-py3-none-any.whl (254 kB)\n","Downloading dython-0.7.11-py3-none-any.whl (27 kB)\n","Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m122.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hDownloading psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m^C\n"]}]},{"cell_type":"code","source":["import os, sys, pathlib, glob\n","# Ensure we're at the repo root (contains \"model/\" and \"model/ctabgan.py\")\n","assert os.path.exists(\"model/ctabgan.py\"), \"Run the unzip cell & cd into the repo root first.\"\n","\n","# Make packages explicit for Python\n","for pkg in [\"model\", \"model/eval\", \"model/synthesizer\"]:\n","    p = pathlib.Path(pkg); p.mkdir(parents=True, exist_ok=True)\n","    ip = p / \"__init__.py\"\n","    if not ip.exists(): ip.write_text(\"\")\n","\n","# Put repo on sys.path\n","if os.getcwd() not in sys.path:\n","    sys.path.insert(0, os.getcwd())\n","\n","from model.ctabgan import CTABGAN\n","print(\"‚úÖ CTABGAN import OK\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"dbupd0JzOD35","executionInfo":{"status":"error","timestamp":1765219168995,"user_tz":480,"elapsed":30,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"2dde975e-932e-4d92-e985-1a5e51e337f7"},"execution_count":2,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"Run the unzip cell & cd into the repo root first.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3220248296.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Ensure we're at the repo root (contains \"model/\" and \"model/ctabgan.py\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/ctabgan.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Run the unzip cell & cd into the repo root first.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Make packages explicit for Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Run the unzip cell & cd into the repo root first."]}]},{"cell_type":"code","source":["# This edits model/synthesizer/ctabgan_synthesizer.py in-place.\n","import re, textwrap, pathlib\n","\n","path = pathlib.Path(\"model/synthesizer/ctabgan_synthesizer.py\")\n","src = path.read_text()\n","\n","# (a) Loss history in __init__\n","if \"self.g_losses\" not in src:\n","    src = re.sub(r\"(def\\s+__init__\\s*\\([^\\)]*\\)\\s*:\\s*\\n)\",\n","                 r\"\\1        # Track loss history\\n        self.g_losses = []\\n        self.d_losses = []\\n\",\n","                 src, count=1)\n","\n","# (b) Print/append losses inside epoch loop\n","if \"##__LOSS_HOOK__\" not in src:\n","    src = src.replace(\"for epoch in range(self.epochs):\",\n","                      \"for epoch in range(self.epochs):\\n            ##__LOSS_HOOK__\")\n","hook = textwrap.dedent(\"\"\"\n","            # Record and print losses each epoch (adjust names if different)\n","            try:\n","                g_val = None; d_val = None\n","                if 'loss_g' in locals(): g_val = float(loss_g.item())\n","                if 'errG'  in locals() and g_val is None: g_val = float(errG.item())\n","                if 'g_loss' in locals() and g_val is None: g_val = float(g_loss.item())\n","                if 'loss_d' in locals(): d_val = float(loss_d.item())\n","                if 'errD'  in locals() and d_val is None: d_val = float(errD.item())\n","                if 'd_loss' in locals() and d_val is None: d_val = float(d_loss.item())\n","                if g_val is not None and d_val is not None:\n","                    self.g_losses.append(g_val); self.d_losses.append(d_val)\n","                    print(f\"Epoch {epoch+1}/{self.epochs} - G: {g_val:.6f}  D: {d_val:.6f}\")\n","            except Exception:\n","                pass\n","\"\"\").rstrip(\"\\n\")\n","src = src.replace(\"##__LOSS_HOOK__\", hook)\n","\n","# (c) Opacus import\n","if \"from opacus import PrivacyEngine\" not in src:\n","    src = src.replace(\"\\nimport\", \"\\nfrom opacus import PrivacyEngine\\nimport\", 1)\n","\n","# (d) DP config in __init__\n","if \"self.dp_enabled\" not in src:\n","    src = re.sub(r\"(def\\s+__init__\\s*\\([^\\)]*\\)\\s*:\\s*\\n)\",\n","                 r\"\\\\1        # Differential Privacy config\\n\"\n","                 r\"        self.dp_enabled = True\\n\"\n","                 r\"        self.dp_noise = 1.0\\n\"\n","                 r\"        self.dp_clip = 1.0\\n\"\n","                 r\"        self.dp_delta = 1e-5\\n\"\n","                 r\"        self._dp_engine = None\\n\", src, count=1)\n","\n","# (e) Wrap discriminator/optimizer/DataLoader with PrivacyEngine.make_private\n","if \"make_private(\" not in src:\n","    # Insert after a likely place where DataLoader is ready.\n","    src = re.sub(r\"(train_loader\\s*=\\s*.*\\n)\", r\"\\\\1\"\n","                 r\"        # Make discriminator private (DP-SGD)\\n\"\n","                 r\"        if self.dp_enabled:\\n\"\n","                 r\"            self._dp_engine = PrivacyEngine()\\n\"\n","                 r\"            self.D, self.optimizerD, train_loader = self._dp_engine.make_private(\\n\"\n","                 r\"                module=self.D,\\n\"\n","                 r\"                optimizer=self.optimizerD,\\n\"\n","                 r\"                data_loader=train_loader,\\n\"\n","                 r\"                noise_multiplier=self.dp_noise,\\n\"\n","                 r\"                max_grad_norm=self.dp_clip,\\n\"\n","                 r\"                poisson_sampling=True,\\n\"\n","                 r\"            )\\n\", src, count=1)\n","\n","# (f) Œµ report each epoch\n","if \"get_epsilon(\" not in src:\n","    src = re.sub(r\"(for\\s+epoch\\s+in\\s+range\\(\\s*self\\.epochs\\s*\\)\\s*:\\s*\\n)\",\n","                 r\"\\\\1            if getattr(self, '_dp_engine', None) is not None:\\n\"\n","                 r\"                try:\\n\"\n","                 r\"                    _eps = self._dp_engine.get_epsilon(delta=self.dp_delta)\\n\"\n","                 r\"                    print(f\\\"[DP] epoch {epoch+1}: Œµ={_eps:.2f}, Œ¥={self.dp_delta}, œÉ={self.dp_noise}, C={self.dp_clip}\\\")\\n\"\n","                 r\"                except Exception:\\n\"\n","                 r\"                    pass\\n\", src, count=1)\n","\n","path.write_text(src)\n","print(\"‚úÖ Patched:\", path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"6bAV5ZDnSZ9D","executionInfo":{"status":"error","timestamp":1765219189545,"user_tz":480,"elapsed":31,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"467390b7-748b-4eaa-dba5-3cfb7bbad3b8"},"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'model/synthesizer/ctabgan_synthesizer.py'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4196364113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model/synthesizer/ctabgan_synthesizer.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# (a) Loss history in __init__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(self, encoding, errors)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \"\"\"\n\u001b[1;32m   1026\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/synthesizer/ctabgan_synthesizer.py'"]}]},{"cell_type":"code","source":["import pandas as pd, os\n","from model.ctabgan import CTABGAN\n","\n","def infer_config(df: pd.DataFrame, label_hint=None):\n","    # Guess label\n","    label = None\n","    for cand in [label_hint, \"Diabetes_binary\", \"Diabetes_012\", \"label\", \"target\"]:\n","        if cand and cand in df.columns:\n","            label = cand; break\n","\n","    categorical_columns = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n","    integer_columns = [c for c in df.columns if pd.api.types.is_integer_dtype(df[c])]\n","    log_columns = []           # add strictly-positive skewed columns if any\n","    mixed_columns = {}         # e.g., {'amount':[0.0]} for zero-inflated positives\n","    general_columns = []       # continuous that are not log/mixed\n","    non_categorical_columns = [c for c in df.columns if c not in categorical_columns]\n","    problem_type = {\"Classification\": label} if label else None\n","\n","    return dict(\n","        categorical_columns=categorical_columns,\n","        integer_columns=integer_columns,\n","        log_columns=log_columns,\n","        mixed_columns=mixed_columns,\n","        general_columns=general_columns,\n","        non_categorical_columns=non_categorical_columns,\n","        problem_type=problem_type\n","    )\n","\n","def train_and_sample(csv_path, dp_noise=1.0, dp_clip=1.0, epochs=50, out_name=None):\n","    df = pd.read_csv(csv_path)\n","    cfg = infer_config(df)\n","    print(\"Label:\", cfg[\"problem_type\"])\n","\n","    g = CTABGAN(\n","        raw_csv_path=csv_path,\n","        test_ratio=0.2,\n","        **{k: cfg[k] for k in [\"categorical_columns\",\"log_columns\",\"mixed_columns\",\n","                               \"general_columns\",\"non_categorical_columns\",\"integer_columns\"]},\n","        problem_type=cfg[\"problem_type\"]\n","    )\n","\n","    # tweak DP settings on synthesizer\n","    g.synthesizer.dp_enabled = True\n","    g.synthesizer.dp_noise   = float(dp_noise)\n","    g.synthesizer.dp_clip    = float(dp_clip)\n","    g.synthesizer.dp_delta   = 1e-5\n","    g.synthesizer.epochs     = int(epochs)\n","\n","    g.fit()\n","    syn = g.generate_samples()          # same #rows as real by default\n","    os.makedirs(\"Fake_Datasets\", exist_ok=True)\n","    out = f\"Fake_Datasets/{out_name or os.path.basename(csv_path).replace('.csv','_synthetic.csv')}\"\n","    syn.to_csv(out, index=False)\n","    print(\"Saved synthetic:\", out)\n","    return out, g\n"],"metadata":{"id":"rUVNu9GbSbup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np, pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.stats import ks_2samp\n","\n","def tvd(p, q):\n","    keys = sorted(set(p.index).union(q.index))\n","    p = p.reindex(keys, fill_value=0).values\n","    q = q.reindex(keys, fill_value=0).values\n","    return 0.5 * np.abs(p - q).sum()\n","\n","def range_coverage(real, syn):\n","    rmin, rmax = np.nanmin(real), np.nanmax(real)\n","    smin, smax = np.nanmin(syn),  np.nanmax(syn)\n","    if not np.isfinite([rmin,rmax,smin,smax]).all() or rmax<=rmin:\n","        return 0.0\n","    span = rmax - rmin\n","    left  = max(0.0, (smin - rmin) / span)\n","    right = max(0.0, (rmax - smax) / span)\n","    return float(np.clip(1.0 - 0.5*(left+right), 0, 1))\n","\n","def anomaly_similarity(real, syn):\n","    qr = np.nanpercentile(real, [25,50,75])\n","    qs = np.nanpercentile(syn,  [25,50,75])\n","    def rate(x, q):\n","        iqr = q[2]-q[0]\n","        if iqr<=0: return 0.0\n","        low, high = q[0]-1.5*iqr, q[2]+1.5*iqr\n","        return float(((x<low)|(x>high)).mean())\n","    rr, rs = rate(real, qr), rate(syn, qs)\n","    return float(1.0 - np.clip(abs(rr-rs), 0, 1))\n","\n","def corr_similarity(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if len(num)<2: return 0.5\n","    Cr = df_r[num].corr().fillna(0).to_numpy()\n","    Cs = df_s[num].corr().fillna(0).to_numpy()\n","    diff = np.linalg.norm(Cr - Cs, ord=\"fro\")\n","    base = np.linalg.norm(np.abs(Cr)+np.abs(Cs), ord=\"fro\") + 1e-8\n","    return float(np.clip(1.0 - diff/base, 0, 1))\n","\n","def category_coverage(df_r, df_s, cols):\n","    if not cols: return 1.0\n","    covs = []\n","    for c in cols:\n","        r = set(df_r[c].astype(str).unique())\n","        s = set(df_s[c].astype(str).unique())\n","        covs.append(len(r & s)/max(1,len(r)))\n","    return float(np.mean(covs))\n","\n","def quantile_diff_score(real, syn, q=100):\n","    qs = np.linspace(0,100,q+1)\n","    r = np.nanpercentile(real, qs); s = np.nanpercentile(syn, qs)\n","    d = np.mean(np.abs(r - s))\n","    scale = np.nanpercentile(real, 75) - np.nanpercentile(real, 25)\n","    if not np.isfinite(scale) or scale<=0: scale = np.nanstd(real) + 1e-8\n","    return float(np.clip(1.0 - d/(scale+1e-8), 0, 1))\n","\n","def dcr_score(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.5\n","    R = df_r[num].dropna().to_numpy()\n","    S = df_s[num].dropna().to_numpy()\n","    if len(R)==0 or len(S)==0: return 0.5\n","    sc = StandardScaler().fit(np.vstack([R,S]))\n","    Rz, Sz = sc.transform(R), sc.transform(S)\n","    nn = NearestNeighbors(n_neighbors=1).fit(Rz)\n","    d_sr = nn.kneighbors(Sz, return_distance=True)[0].ravel()\n","    d_rr = nn.kneighbors(Rz, return_distance=True)[0].ravel()\n","    shift = np.median(d_sr) - np.median(d_rr)\n","    return float(1.0/(1.0+np.exp(-4.0*(shift-0.05))))  # larger shift ‚áí more private\n","\n","def duplicate_overlap(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.0\n","    R = df_r[num].fillna(0).to_numpy()\n","    S = df_s[num].fillna(0).to_numpy()\n","    R = (R - R.min(0))/(R.ptp(0)+1e-8); S = (S - S.min(0))/(S.ptp(0)+1e-8)\n","    R = (R*64).astype(int); S = (S*64).astype(int)\n","    rkeys = {tuple(row) for row in R}\n","    skeys = {tuple(row) for row in S}\n","    ov = len(rkeys & skeys)/max(1,len(skeys))  # lower is better\n","    return float(ov)\n","\n","def rate_QP(real_csv, syn_csv, dp_sigma_release=0.0):\n","    real = pd.read_csv(real_csv); syn = pd.read_csv(syn_csv)\n","\n","    cat_cols = real.select_dtypes(include=['object','category','bool']).columns.tolist()\n","    num_cols = [c for c in real.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(real[c])]\n","\n","    # Quality\n","    ks_vals = [ks_2samp(real[c].dropna().astype(float), syn[c].dropna().astype(float)).statistic\n","               for c in num_cols if len(real[c].dropna())>30 and len(syn[c].dropna())>30]\n","    Q_num = 1.0 - (np.mean(ks_vals) if ks_vals else 1.0)\n","\n","    tvd_vals = [tvd(real[c].value_counts(normalize=True), syn[c].value_counts(normalize=True)) for c in cat_cols]\n","    Q_cat = 1.0 - (np.mean(tvd_vals) if tvd_vals else 0.0)\n","\n","    Q_range = np.mean([range_coverage(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","    Q_anom  = np.mean([anomaly_similarity(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","    Q_corr  = corr_similarity(real, syn)\n","    Q_cov   = category_coverage(real, syn, cat_cols)\n","\n","    w = dict(num=0.20, cat=0.20, range=0.15, anom=0.15, corr=0.20, cov=0.10)\n","    Q = (w['num']*Q_num + w['cat']*Q_cat + w['range']*Q_range +\n","         w['anom']*Q_anom + w['corr']*Q_corr + w['cov']*Q_cov)\n","\n","    # Privacy\n","    P_dcr = dcr_score(real, syn)\n","    qd_vals = [quantile_diff_score(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]\n","    P_qd = np.mean(qd_vals) if qd_vals else 0.5\n","    P_dup = 1.0 - np.clip(duplicate_overlap(real, syn), 0, 1)\n","    wp = dict(dcr=0.5, qd=0.3, dup=0.2)\n","    P = wp['dcr']*P_dcr + wp['qd']*P_qd + wp['dup']*P_dup\n","\n","    return dict(Q_overall=Q, Q_num=Q_num, Q_cat=Q_cat, Q_range=Q_range, Q_anom=Q_anom, Q_corr=Q_corr, Q_div=Q_cov,\n","                P_overall=P, P_DCR=P_dcr, P_Qdelta=P_qd, P_noDup=P_dup)\n"],"metadata":{"id":"xJShba0JSiiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","\n","def auto_label(df: pd.DataFrame, label_hint=None):\n","    \"\"\"\n","    Return the name of the label column for CTAB-GAN+ classification.\n","    Tries explicit hint, common diabetes names, and a heuristic (name contains 'diabetes' and low cardinality).\n","    Raises with a helpful message if nothing fits.\n","    \"\"\"\n","    # normalize column names (strip spaces)\n","    df = df.copy()\n","    df.columns = df.columns.str.strip()\n","\n","    # 1) explicit hint\n","    if label_hint and label_hint in df.columns:\n","        return label_hint\n","\n","    # 2) common diabetes labels\n","    common = [\"Diabetes_binary\", \"diabetes_binary\", \"Diabetes_012\", \"diabetes_012\", \"Diabetes\"]\n","    for c in df.columns:\n","        if c in common:\n","            # must be classification-like cardinality\n","            if df[c].nunique(dropna=True) in (2, 3):\n","                return c\n","\n","    # 3) heuristic: column name contains 'diabetes' and is 2 or 3 classes\n","    for c in df.columns:\n","        if \"diab\" in c.lower() and df[c].nunique(dropna=True) in (2, 3):\n","            return c\n","\n","    # 4) last-resort: smallest-cardinality non-constant column (but still classification-like)\n","    candidates = [(c, df[c].nunique(dropna=True)) for c in df.columns]\n","    candidates = [(c, k) for c, k in candidates if 1 < k <= 10]  # avoid continuous\n","    if candidates:\n","        candidates.sort(key=lambda x: x[1])\n","        # prefer exactly 2 or 3 classes if present\n","        for c,k in candidates:\n","            if k in (2,3):\n","                return c\n","        # otherwise take the smallest discrete\n","        return candidates[0][0]\n","\n","    raise ValueError(\n","        \"Could not infer a classification label column. \"\n","        \"Pass label_hint='Diabetes_binary' or 'Diabetes_012' to train_and_sample(...).\"\n","    )\n","\n","def infer_config(df: pd.DataFrame, label_hint=None):\n","    df = df.copy()\n","    df.columns = df.columns.str.strip()\n","\n","    # Pick the label (binary or 3-class)\n","    label = auto_label(df, label_hint=label_hint)\n","\n","    # Ensure label is integer-coded (0/1 or 0/1/2)\n","    if not pd.api.types.is_integer_dtype(df[label]):\n","        # try safe cast for typical 0.0/1.0 or strings like \"0\"/\"1\"\n","        try:\n","            df[label] = pd.to_numeric(df[label], errors=\"raise\").round().astype(int)\n","        except Exception:\n","            # fallback: category codes\n","            df[label] = df[label].astype(\"category\").cat.codes\n","\n","    # Build column type lists\n","    categorical_columns = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n","    integer_columns = [c for c in df.columns if pd.api.types.is_integer_dtype(df[c])]\n","    log_columns = []\n","    mixed_columns = {}\n","    general_columns = []\n","    non_categorical_columns = [c for c in df.columns if c not in categorical_columns]\n","\n","    return dict(\n","        problem_type={\"Classification\": label},\n","        categorical_columns=categorical_columns,\n","        integer_columns=integer_columns,\n","        log_columns=log_columns,\n","        mixed_columns=mixed_columns,\n","        general_columns=general_columns,\n","        non_categorical_columns=non_categorical_columns\n","    )\n","\n","def train_and_sample(csv_path, CTABGAN_cls, dp_noise=1.0, dp_clip=1.0, epochs=60, out_name=None, label_hint=None):\n","    df = pd.read_csv(csv_path)\n","    cfg = infer_config(df, label_hint=label_hint)\n","    print(\"Label detected:\", cfg[\"problem_type\"])\n","\n","    g = CTABGAN_cls(\n","        raw_csv_path=csv_path,\n","        test_ratio=0.2,\n","        categorical_columns=cfg[\"categorical_columns\"],\n","        integer_columns=cfg[\"integer_columns\"],\n","        log_columns=cfg[\"log_columns\"],\n","        mixed_columns=cfg[\"mixed_columns\"],\n","        general_columns=cfg[\"general_columns\"],\n","        non_categorical_columns=cfg[\"non_categorical_columns\"],\n","        problem_type=cfg[\"problem_type\"],   # üëà always non-None now\n","    )\n","\n","    # DP knobs on the synthesizer (assumes your DP patch is in place)\n","    s = g.synthesizer\n","    s.dp_enabled = True\n","    s.dp_noise   = float(dp_noise)\n","    s.dp_clip    = float(dp_clip)\n","    s.dp_delta   = 1e-5\n","    s.epochs     = int(epochs)\n","\n","    g.fit()\n","    syn = g.generate_samples()\n","    out_path = out_name or (csv_path.rsplit(\".\",1)[0] + \"_CTABGAN_DP.csv\")\n","    pd.DataFrame(syn).to_csv(out_path, index=False)\n","    print(\"Saved synthetic:\", out_path)\n","    return out_path, g\n"],"metadata":{"id":"83STvYIMCqoz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from model.ctabgan import CTABGAN\n","\n","real_csvs = [\n","    \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\",\n","]\n","\n","# Provide hints so we pick the intended label quickly\n","hints = {\n","    \"diabetes_binary_health_indicators_BRFSS2015.csv\": \"Diabetes_binary\",\n","    \"diabetes_binary_5050split_health_indicators_BRFSS2015.csv\": \"Diabetes_binary\",\n","    \"diabetes_012_health_indicators_BRFSS2015.csv\": \"Diabetes_012\",\n","}\n","\n","DP_NOISE, DP_CLIP, EPOCHS = 1.0, 1.0, 80\n","\n","for rp in real_csvs:\n","    hint = None\n","    for k,v in hints.items():\n","        if k in rp:\n","            hint = v\n","            break\n","    out_syn, model = train_and_sample(\n","        csv_path=rp,\n","        CTABGAN_cls=CTABGAN,\n","        dp_noise=DP_NOISE,\n","        dp_clip=DP_CLIP,\n","        epochs=EPOCHS,\n","        out_name=rp.replace(\".csv\", \"_CTABGAN_DP.csv\"),\n","        label_hint=hint,   # üëà this ensures `problem_type` is set\n","    )\n"],"metadata":{"id":"5N0jKn5WCtPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q (quality) & P (privacy) ratings for Breast Cancer Wisconsin (real vs CTAB-GAN+ DP synthetic)\n","\n","import numpy as np, pandas as pd\n","from pathlib import Path\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.stats import ks_2samp\n","\n","# --- set your paths (update if your files live elsewhere) ---\n","real_path = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"\n","syn_path  = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin_CTABGAN_DP.csv\"\n","\n","real = pd.read_csv(real_path)\n","syn  = pd.read_csv(syn_path)\n","\n","# ---------- helpers ----------\n","def tvd(p, q):\n","    keys = sorted(set(p.index).union(q.index))\n","    p = p.reindex(keys, fill_value=0).values\n","    q = q.reindex(keys, fill_value=0).values\n","    return 0.5 * np.abs(p - q).sum()\n","\n","def range_coverage(real, syn):\n","    rmin, rmax = np.nanmin(real), np.nanmax(real)\n","    smin, smax = np.nanmin(syn),  np.nanmax(syn)\n","    if not np.isfinite([rmin,rmax,smin,smax]).all() or rmax<=rmin:\n","        return 0.0\n","    span = rmax - rmin\n","    left  = max(0.0, (smin - rmin) / span)\n","    right = max(0.0, (rmax - smax) / span)\n","    return float(np.clip(1.0 - 0.5*(left+right), 0, 1))\n","\n","def anomaly_similarity(real, syn):\n","    qr = np.nanpercentile(real, [25,50,75])\n","    qs = np.nanpercentile(syn,  [25,50,75])\n","    def rate(x, q):\n","        iqr = q[2]-q[0]\n","        if iqr<=0: return 0.0\n","        low, high = q[0]-1.5*iqr, q[2]+1.5*iqr\n","        return float(((x<low)|(x>high)).mean())\n","    rr, rs = rate(real, qr), rate(syn, qs)\n","    return float(1.0 - np.clip(abs(rr-rs), 0, 1))\n","\n","def corr_similarity(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if len(num)<2: return 0.5\n","    Cr = df_r[num].corr().fillna(0).to_numpy()\n","    Cs = df_s[num].corr().fillna(0).to_numpy()\n","    diff = np.linalg.norm(Cr - Cs, ord=\"fro\")\n","    base = np.linalg.norm(np.abs(Cr)+np.abs(Cs), ord=\"fro\") + 1e-8\n","    return float(np.clip(1.0 - diff/base, 0, 1))\n","\n","def category_coverage(df_r, df_s, cols):\n","    if not cols: return 1.0\n","    covs = []\n","    for c in cols:\n","        r = set(df_r[c].astype(str).unique())\n","        s = set(df_s[c].astype(str).unique())\n","        covs.append(len(r & s)/max(1,len(r)))\n","    return float(np.mean(covs))\n","\n","def quantile_diff_score(real, syn, q=100):\n","    qs = np.linspace(0,100,q+1)\n","    r = np.nanpercentile(real, qs); s = np.nanpercentile(syn, qs)\n","    d = np.mean(np.abs(r - s))\n","    scale = np.nanpercentile(real, 75) - np.nanpercentile(real, 25)\n","    if not np.isfinite(scale) or scale<=0: scale = np.nanstd(real) + 1e-8\n","    return float(np.clip(1.0 - d/(scale+1e-8), 0, 1))\n","\n","def dcr_score(df_r, df_s):\n","    # Distance to closest real record shift (synthetic‚Üíreal vs real‚Üíreal); larger shift = more private\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.5\n","    R = df_r[num].dropna().to_numpy()\n","    S = df_s[num].dropna().to_numpy()\n","    if len(R)==0 or len(S)==0: return 0.5\n","    sc = StandardScaler().fit(np.vstack([R,S]))\n","    Rz, Sz = sc.transform(R), sc.transform(S)\n","    nn = NearestNeighbors(n_neighbors=1).fit(Rz)\n","    d_sr = nn.kneighbors(Sz, return_distance=True)[0].ravel()\n","    d_rr = nn.kneighbors(Rz, return_distance=True)[0].ravel()\n","    shift = np.median(d_sr) - np.median(d_rr)\n","    return float(1.0/(1.0+np.exp(-4.0*(shift-0.05))))\n","\n","def duplicate_overlap(df_r, df_s):\n","    # Rough near-duplicate proxy on numeric features via coarse binning\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.0\n","    R = df_r[num].fillna(0).to_numpy()\n","    S = df_s[num].fillna(0).to_numpy()\n","    R = (R - R.min(0))/(R.ptp(0)+1e-8); S = (S - S.min(0))/(S.ptp(0)+1e-8)\n","    R = (R*64).astype(int); S = (S*64).astype(int)\n","    rkeys = {tuple(row) for row in R}\n","    skeys = {tuple(row) for row in S}\n","    ov = len(rkeys & skeys)/max(1,len(skeys))  # lower is better\n","    return float(ov)\n","\n","# ---------- compute Q ----------\n","cat_cols = real.select_dtypes(include=['object','category','bool']).columns.tolist()\n","num_cols = [c for c in real.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(real[c])]\n","\n","ks_vals = []\n","for c in num_cols:\n","    r = real[c].dropna().astype(float); s = syn[c].dropna().astype(float)\n","    if len(r)>30 and len(s)>30:\n","        ks_vals.append(ks_2samp(r, s).statistic)\n","Q_num = 1.0 - (np.mean(ks_vals) if ks_vals else 1.0)\n","\n","tvd_vals = []\n","for c in cat_cols:\n","    tvd_vals.append(tvd(real[c].value_counts(normalize=True), syn[c].value_counts(normalize=True)))\n","Q_cat = 1.0 - (np.mean(tvd_vals) if tvd_vals else 0.0)\n","\n","Q_range = np.mean([range_coverage(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_anom  = np.mean([anomaly_similarity(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_corr  = corr_similarity(real, syn)\n","Q_div   = category_coverage(real, syn, cat_cols)\n","\n","w = dict(num=0.20, cat=0.20, range=0.15, anom=0.15, corr=0.20, cov=0.10)\n","Q_overall = (w['num']*Q_num + w['cat']*Q_cat + w['range']*Q_range +\n","             w['anom']*Q_anom + w['corr']*Q_corr + w['cov']*Q_div)\n","\n","# ---------- compute P ----------\n","P_DCR    = dcr_score(real, syn)\n","P_Qdelta = np.mean([quantile_diff_score(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 0.5\n","P_noDup  = 1.0 - np.clip(duplicate_overlap(real, syn), 0, 1)\n","wp = dict(dcr=0.5, qd=0.3, dup=0.2)\n","P_overall = wp['dcr']*P_DCR + wp['qd']*P_Qdelta + wp['dup']*P_noDup\n","\n","# ---------- report ----------\n","report = pd.DataFrame([{\n","    \"dataset\": Path(real_path).name,\n","    \"synthetic\": Path(syn_path).name,\n","    \"Q_overall\": round(Q_overall,3),\n","    \"Q_num(1-KS)\": round(Q_num,3),\n","    \"Q_cat(1-TVD)\": round(Q_cat,3),\n","    \"Q_range\": round(Q_range,3),\n","    \"Q_anomaly\": round(Q_anom,3),\n","    \"Q_corr\": round(Q_corr,3),\n","    \"Q_diversity\": round(Q_div,3),\n","    \"P_overall\": round(P_overall,3),\n","    \"P_DCR\": round(P_DCR,3),\n","    \"P_Qdelta\": round(P_Qdelta,3),\n","    \"P_noDup\": round(P_noDup,3),\n","    \"n_real\": len(real),\n","    \"n_syn\": len(syn),\n","    \"num_cols_used\": len(num_cols),\n","    \"cat_cols_used\": len(cat_cols)\n","}])\n","print(report.to_string(index=False))\n","\n","# Save for download\n","out_csv = \"/mnt/data/breast_cancer_CTABGAN_DP_QP_report.csv\"\n","report.to_csv(out_csv, index=False)\n","print(\"\\nSaved report to:\", out_csv)\n"],"metadata":{"id":"5ikrQDwZFoPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Q (quality) & P (privacy) ratings for Real vs CTAB-GAN+ (DP-SGD) Synthetic\n","# --- Set your paths here ---\n","real_path = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"\n","syn_path  = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin_CTABGAN_DP.csv\"\n","out_csv   = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast_cancer_CTABGAN_DP_QP_report.csv\"\n","\n","import numpy as np, pandas as pd\n","from pathlib import Path\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.stats import ks_2samp\n","\n","# ---------- helpers ----------\n","def tvd(p, q):\n","    keys = sorted(set(p.index).union(q.index))\n","    p = p.reindex(keys, fill_value=0).values\n","    q = q.reindex(keys, fill_value=0).values\n","    return 0.5 * np.abs(p - q).sum()\n","\n","def range_coverage(real, syn):\n","    rmin, rmax = np.nanmin(real), np.nanmax(real)\n","    smin, smax = np.nanmin(syn),  np.nanmax(syn)\n","    if not np.isfinite([rmin,rmax,smin,smax]).all() or rmax<=rmin:\n","        return 0.0\n","    span = rmax - rmin\n","    left  = max(0.0, (smin - rmin) / span)\n","    right = max(0.0, (rmax - smax) / span)\n","    return float(np.clip(1.0 - 0.5*(left+right), 0, 1))\n","\n","def anomaly_similarity(real, syn):\n","    qr = np.nanpercentile(real, [25,50,75])\n","    qs = np.nanpercentile(syn,  [25,50,75])\n","    def rate(x, q):\n","        iqr = q[2]-q[0]\n","        if iqr<=0: return 0.0\n","        low, high = q[0]-1.5*iqr, q[2]+1.5*iqr\n","        return float(((x<low)|(x>high)).mean())\n","    rr, rs = rate(real, qr), rate(syn, qs)\n","    return float(1.0 - np.clip(abs(rr-rs), 0, 1))\n","\n","def corr_similarity(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if len(num)<2: return 0.5\n","    Cr = df_r[num].corr().fillna(0).to_numpy()\n","    Cs = df_s[num].corr().fillna(0).to_numpy()\n","    diff = np.linalg.norm(Cr - Cs, ord=\"fro\")\n","    base = np.linalg.norm(np.abs(Cr)+np.abs(Cs), ord=\"fro\") + 1e-8\n","    return float(np.clip(1.0 - diff/base, 0, 1))\n","\n","def category_coverage(df_r, df_s, cols):\n","    if not cols: return 1.0\n","    covs = []\n","    for c in cols:\n","        r = set(df_r[c].astype(str).unique())\n","        s = set(df_s[c].astype(str).unique())\n","        covs.append(len(r & s)/max(1,len(r)))\n","    return float(np.mean(covs))\n","\n","def quantile_diff_score(real, syn, q=100):\n","    qs = np.linspace(0,100,q+1)\n","    r = np.nanpercentile(real, qs); s = np.nanpercentile(syn, qs)\n","    d = np.mean(np.abs(r - s))\n","    scale = np.nanpercentile(real, 75) - np.nanpercentile(real, 25)\n","    if not np.isfinite(scale) or scale<=0: scale = np.nanstd(real) + 1e-8\n","    return float(np.clip(1.0 - d/(scale+1e-8), 0, 1))\n","\n","def dcr_score(df_r, df_s):\n","    # Distance to closest real record shift (synthetic‚Üíreal vs real‚Üíreal); larger shift = more private\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.5\n","    R = df_r[num].dropna().to_numpy()\n","    S = df_s[num].dropna().to_numpy()\n","    if len(R)==0 or len(S)==0: return 0.5\n","    sc = StandardScaler().fit(np.vstack([R,S]))\n","    Rz, Sz = sc.transform(R), sc.transform(S)\n","    nn = NearestNeighbors(n_neighbors=1).fit(Rz)\n","    d_sr = nn.kneighbors(Sz, return_distance=True)[0].ravel()\n","    d_rr = nn.kneighbors(Rz, return_distance=True)[0].ravel()\n","    shift = np.median(d_sr) - np.median(d_rr)\n","    return float(1.0/(1.0+np.exp(-4.0*(shift-0.05))))\n","\n","def duplicate_overlap(df_r, df_s):\n","    \"\"\"\n","    Rough near-duplicate proxy on numeric features via coarse binning.\n","    **NumPy 2.x safe**: uses np.ptp(..., axis=0) instead of ndarray.ptp(...)\n","    \"\"\"\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.0\n","    R = df_r[num].fillna(0).to_numpy()\n","    S = df_s[num].fillna(0).to_numpy()\n","    # --- minimal change per your error: use np.ptp instead of R.ptp / S.ptp ---\n","    R = (R - R.min(0)) / (np.ptp(R, axis=0) + 1e-8)\n","    S = (S - S.min(0)) / (np.ptp(S, axis=0) + 1e-8)\n","    R = (R*64).astype(int); S = (S*64).astype(int)\n","    rkeys = {tuple(row) for row in R}\n","    skeys = {tuple(row) for row in S}\n","    ov = len(rkeys & skeys)/max(1,len(skeys))  # lower is better\n","    return float(ov)\n","\n","# ---------- load data ----------\n","real = pd.read_csv(real_path)\n","syn  = pd.read_csv(syn_path)\n","\n","# ---------- compute Q ----------\n","cat_cols = real.select_dtypes(include=['object','category','bool']).columns.tolist()\n","num_cols = [c for c in real.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(real[c])]\n","\n","ks_vals = []\n","for c in num_cols:\n","    r = real[c].dropna().astype(float); s = syn[c].dropna().astype(float)\n","    if len(r)>30 and len(s)>30:\n","        ks_vals.append(ks_2samp(r, s).statistic)\n","Q_num = 1.0 - (np.mean(ks_vals) if ks_vals else 1.0)\n","\n","tvd_vals = []\n","for c in cat_cols:\n","    tvd_vals.append(tvd(real[c].value_counts(normalize=True), syn[c].value_counts(normalize=True)))\n","Q_cat = 1.0 - (np.mean(tvd_vals) if tvd_vals else 0.0)\n","\n","Q_range = np.mean([range_coverage(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_anom  = np.mean([anomaly_similarity(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_corr  = corr_similarity(real, syn)\n","Q_div   = category_coverage(real, syn, cat_cols)\n","\n","w = dict(num=0.20, cat=0.20, range=0.15, anom=0.15, corr=0.20, cov=0.10)\n","Q_overall = (w['num']*Q_num + w['cat']*Q_cat + w['range']*Q_range +\n","             w['anom']*Q_anom + w['corr']*Q_corr + w['cov']*Q_div)\n","\n","# ---------- compute P ----------\n","P_DCR    = dcr_score(real, syn)\n","P_Qdelta = np.mean([quantile_diff_score(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 0.5\n","P_noDup  = 1.0 - np.clip(duplicate_overlap(real, syn), 0, 1)\n","wp = dict(dcr=0.5, qd=0.3, dup=0.2)\n","P_overall = wp['dcr']*P_DCR + wp['qd']*P_Qdelta + wp['dup']*P_noDup\n","\n","# ---------- report ----------\n","report = pd.DataFrame([{\n","    \"dataset\": Path(real_path).name,\n","    \"synthetic\": Path(syn_path).name,\n","    \"Q_overall\": round(Q_overall,3),\n","    \"Q_num(1-KS)\": round(Q_num,3),\n","    \"Q_cat(1-TVD)\": round(Q_cat,3),\n","    \"Q_range\": round(Q_range,3),\n","    \"Q_anomaly\": round(Q_anom,3),\n","    \"Q_corr\": round(Q_corr,3),\n","    \"Q_diversity\": round(Q_div,3),\n","    \"P_overall\": round(P_overall,3),\n","    \"P_DCR\": round(P_DCR,3),\n","    \"P_Qdelta\": round(P_Qdelta,3),\n","    \"P_noDup\": round(P_noDup,3),\n","    \"n_real\": len(real),\n","    \"n_syn\": len(syn),\n","    \"num_cols_used\": len(num_cols),\n","    \"cat_cols_used\": len(cat_cols)\n","}])\n","\n","print(report.to_string(index=False))\n","report.to_csv(out_csv, index=False)\n","print(\"\\nSaved report to:\", out_csv)\n"],"metadata":{"id":"VwrwOdwZGknQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot CTAB-GAN+ (DP-SGD) loss curves for Breast Cancer dataset\n","# Tries (1) in-memory losses on the trained model; else (2) CSV logs on disk.\n","\n","import os, sys, glob, json, pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def _has_attr_chain(obj, chain):\n","    cur = obj\n","    for name in chain.split('.'):\n","        if not hasattr(cur, name):\n","            return False\n","        cur = getattr(cur, name)\n","    return True\n","\n","def _get_attr_chain(obj, chain):\n","    cur = obj\n","    for name in chain.split('.'):\n","        cur = getattr(cur, name)\n","    return cur\n","\n","def load_losses_from_memory():\n","    # Accept either `ctab` or `model` variable names\n","    for varname in (\"ctab\", \"model\"):\n","        if varname in globals():\n","            obj = globals()[varname]\n","            if _has_attr_chain(obj, \"synthesizer.g_losses\") and _has_attr_chain(obj, \"synthesizer.d_losses\"):\n","                g = _get_attr_chain(obj, \"synthesizer.g_losses\")\n","                d = _get_attr_chain(obj, \"synthesizer.d_losses\")\n","                if isinstance(g, (list, tuple)) and isinstance(d, (list, tuple)) and len(g) and len(d):\n","                    return list(range(1, 1+min(len(g), len(d)))), np.array(g[:len(d)]), np.array(d[:len(g)])\n","    return None\n","\n","def load_losses_from_csv():\n","    candidates = []\n","    # Common places/names; add your path if you saved elsewhere\n","    candidates += glob.glob(\"outputs/*loss*.csv\")\n","    candidates += glob.glob(\"Fake_Datasets/*loss*.csv\")\n","    candidates += glob.glob(\"**/*loss*.csv\", recursive=True)\n","    candidates = [p for p in candidates if os.path.isfile(p)]\n","    for p in sorted(candidates):\n","        try:\n","            df = pd.read_csv(p)\n","            cols = [c.lower() for c in df.columns]\n","            # Expect epoch, g_loss, d_loss (case-insensitive)\n","            def col(name):\n","                for c in df.columns:\n","                    if c.lower()==name: return c\n","                return None\n","            ec = col(\"epoch\"); gc = col(\"g_loss\"); dc = col(\"d_loss\")\n","            if ec and gc and dc:\n","                e = df[ec].to_numpy()\n","                g = df[gc].to_numpy()\n","                d = df[dc].to_numpy()\n","                k = int(min(len(e), len(g), len(d)))\n","                if k>0:\n","                    return e[:k], g[:k], d[:k], p\n","        except Exception:\n","            continue\n","    return None\n","\n","loaded = load_losses_from_memory()\n","source = \"memory\"\n","csv_path = None\n","if loaded is None:\n","    csv_loaded = load_losses_from_csv()\n","    if csv_loaded is not None:\n","        e, g, d, csv_path = csv_loaded\n","        source = f\"csv:{csv_path}\"\n","    else:\n","        e = g = d = None\n","\n","if e is None:\n","    msg = (\n","        \"No loss history found.\\n\\n\"\n","        \"I looked for:\\n\"\n","        \"  1) an in-memory trained object named `ctab` or `model` with `synthesizer.g_losses` and `synthesizer.d_losses`\\n\"\n","        \"  2) a CSV like `*loss*.csv` containing columns: epoch,g_loss,d_loss\\n\\n\"\n","        \"If you patched the repo to record losses, make sure to:\\n\"\n","        \"  - keep your trained object alive (e.g., variable `ctab`), or\\n\"\n","        \"  - write the losses to CSV during training and point me to that file.\\n\"\n","    )\n","    raise RuntimeError(msg)\n","\n","# If we got memory-based losses, unpack them now\n","if loaded is not None:\n","    e, g, d = loaded\n","\n","# Plot one chart with both losses\n","plt.figure()\n","plt.plot(e, g, label=\"G (generator)\")\n","plt.plot(e, d, label=\"D (discriminator)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"CTAB-GAN+ (DP-SGD) ‚Äî Training Loss (Breast Cancer)\")\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"Loss source: {source}\")\n","if csv_path:\n","    print(f\"Used log file: {csv_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"jyoSMeNNKPf3","executionInfo":{"status":"error","timestamp":1765219235858,"user_tz":480,"elapsed":1368,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"551cb95d-0752-4ad7-f727-a73491728eaa"},"execution_count":4,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"No loss history found.\n\nI looked for:\n  1) an in-memory trained object named `ctab` or `model` with `synthesizer.g_losses` and `synthesizer.d_losses`\n  2) a CSV like `*loss*.csv` containing columns: epoch,g_loss,d_loss\n\nIf you patched the repo to record losses, make sure to:\n  - keep your trained object alive (e.g., variable `ctab`), or\n  - write the losses to CSV during training and point me to that file.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1308467837.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"  - write the losses to CSV during training and point me to that file.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# If we got memory-based losses, unpack them now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No loss history found.\n\nI looked for:\n  1) an in-memory trained object named `ctab` or `model` with `synthesizer.g_losses` and `synthesizer.d_losses`\n  2) a CSV like `*loss*.csv` containing columns: epoch,g_loss,d_loss\n\nIf you patched the repo to record losses, make sure to:\n  - keep your trained object alive (e.g., variable `ctab`), or\n  - write the losses to CSV during training and point me to that file.\n"]}]},{"cell_type":"code","source":["# Q (quality) & P (privacy) ratings for Real vs CTAB-GAN+ (DP-SGD) Synthetic\n","# --- Set your paths here ---\n","real_path = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin.csv\"\n","syn_path  = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast-cancer-wisconsin_CTABGAN_DP.csv\"\n","out_csv   = \"/content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast_cancer_CTABGAN_DP_QP_report.csv\"\n","\n","import numpy as np, pandas as pd\n","from pathlib import Path\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import NearestNeighbors\n","from scipy.stats import ks_2samp\n","\n","# ---------- helpers ----------\n","def tvd(p, q):\n","    keys = sorted(set(p.index).union(q.index))\n","    p = p.reindex(keys, fill_value=0).values\n","    q = q.reindex(keys, fill_value=0).values\n","    return 0.5 * np.abs(p - q).sum()\n","\n","def range_coverage(real, syn):\n","    rmin, rmax = np.nanmin(real), np.nanmax(real)\n","    smin, smax = np.nanmin(syn),  np.nanmax(syn)\n","    if not np.isfinite([rmin,rmax,smin,smax]).all() or rmax<=rmin:\n","        return 0.0\n","    span = rmax - rmin\n","    left  = max(0.0, (smin - rmin) / span)\n","    right = max(0.0, (rmax - smax) / span)\n","    return float(np.clip(1.0 - 0.5*(left+right), 0, 1))\n","\n","def anomaly_similarity(real, syn):\n","    qr = np.nanpercentile(real, [25,50,75])\n","    qs = np.nanpercentile(syn,  [25,50,75])\n","    def rate(x, q):\n","        iqr = q[2]-q[0]\n","        if iqr<=0: return 0.0\n","        low, high = q[0]-1.5*iqr, q[2]+1.5*iqr\n","        return float(((x<low)|(x>high)).mean())\n","    rr, rs = rate(real, qr), rate(syn, qs)\n","    return float(1.0 - np.clip(abs(rr-rs), 0, 1))\n","\n","def corr_similarity(df_r, df_s):\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if len(num)<2: return 0.5\n","    Cr = df_r[num].corr().fillna(0).to_numpy()\n","    Cs = df_s[num].corr().fillna(0).to_numpy()\n","    diff = np.linalg.norm(Cr - Cs, ord=\"fro\")\n","    base = np.linalg.norm(np.abs(Cr)+np.abs(Cs), ord=\"fro\") + 1e-8\n","    return float(np.clip(1.0 - diff/base, 0, 1))\n","\n","def category_coverage(df_r, df_s, cols):\n","    if not cols: return 1.0\n","    covs = []\n","    for c in cols:\n","        r = set(df_r[c].astype(str).unique())\n","        s = set(df_s[c].astype(str).unique())\n","        covs.append(len(r & s)/max(1,len(r)))\n","    return float(np.mean(covs))\n","\n","def quantile_diff_score(real, syn, q=100):\n","    qs = np.linspace(0,100,q+1)\n","    r = np.nanpercentile(real, qs); s = np.nanpercentile(syn, qs)\n","    d = np.mean(np.abs(r - s))\n","    scale = np.nanpercentile(real, 75) - np.nanpercentile(real, 25)\n","    if not np.isfinite(scale) or scale<=0: scale = np.nanstd(real) + 1e-8\n","    return float(np.clip(1.0 - d/(scale+1e-8), 0, 1))\n","\n","def dcr_score(df_r, df_s):\n","    # Distance to closest real record shift (synthetic‚Üíreal vs real‚Üíreal); larger shift = more private\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.5\n","    R = df_r[num].dropna().to_numpy()\n","    S = df_s[num].dropna().to_numpy()\n","    if len(R)==0 or len(S)==0: return 0.5\n","    sc = StandardScaler().fit(np.vstack([R,S]))\n","    Rz, Sz = sc.transform(R), sc.transform(S)\n","    nn = NearestNeighbors(n_neighbors=1).fit(Rz)\n","    d_sr = nn.kneighbors(Sz, return_distance=True)[0].ravel()\n","    d_rr = nn.kneighbors(Rz, return_distance=True)[0].ravel()\n","    shift = np.median(d_sr) - np.median(d_rr)\n","    return float(1.0/(1.0+np.exp(-4.0*(shift-0.05))))\n","\n","def duplicate_overlap(df_r, df_s):\n","    \"\"\"\n","    Rough near-duplicate proxy on numeric features via coarse binning.\n","    **NumPy 2.x safe**: uses np.ptp(..., axis=0) instead of ndarray.ptp(...).\n","    \"\"\"\n","    num = df_r.select_dtypes(include=[np.number]).columns.tolist()\n","    if not num: return 0.0\n","    R = df_r[num].fillna(0).to_numpy()\n","    S = df_s[num].fillna(0).to_numpy()\n","    R = (R - R.min(0)) / (np.ptp(R, axis=0) + 1e-8)\n","    S = (S - S.min(0)) / (np.ptp(S, axis=0) + 1e-8)\n","    R = (R*64).astype(int); S = (S*64).astype(int)\n","    rkeys = {tuple(row) for row in R}\n","    skeys = {tuple(row) for row in S}\n","    ov = len(rkeys & skeys)/max(1,len(skeys))  # lower is better\n","    return float(ov)\n","\n","# ---------- load data ----------\n","real = pd.read_csv(real_path)\n","syn  = pd.read_csv(syn_path)\n","\n","# ---------- compute Q ----------\n","cat_cols = real.select_dtypes(include=['object','category','bool']).columns.tolist()\n","num_cols = [c for c in real.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(real[c])]\n","\n","ks_vals = []\n","for c in num_cols:\n","    r = real[c].dropna().astype(float); s = syn[c].dropna().astype(float)\n","    if len(r)>30 and len(s)>30:\n","        ks_vals.append(ks_2samp(r, s).statistic)\n","Q_num = 1.0 - (np.mean(ks_vals) if ks_vals else 1.0)\n","\n","tvd_vals = []\n","for c in cat_cols:\n","    tvd_vals.append(tvd(real[c].value_counts(normalize=True), syn[c].value_counts(normalize=True)))\n","Q_cat = 1.0 - (np.mean(tvd_vals) if tvd_vals else 0.0)\n","\n","Q_range = np.mean([range_coverage(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_anom  = np.mean([anomaly_similarity(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 1.0\n","Q_corr  = corr_similarity(real, syn)\n","Q_div   = category_coverage(real, syn, cat_cols)\n","\n","w = dict(num=0.20, cat=0.20, range=0.15, anom=0.15, corr=0.20, cov=0.10)\n","Q_overall = (w['num']*Q_num + w['cat']*Q_cat + w['range']*Q_range +\n","             w['anom']*Q_anom + w['corr']*Q_corr + w['cov']*Q_div)\n","\n","# ---------- compute P ----------\n","P_DCR    = dcr_score(real, syn)\n","P_Qdelta = np.mean([quantile_diff_score(real[c].dropna().to_numpy(), syn[c].dropna().to_numpy()) for c in num_cols]) if num_cols else 0.5\n","P_noDup  = 1.0 - np.clip(duplicate_overlap(real, syn), 0, 1)\n","wp = dict(dcr=0.5, qd=0.3, dup=0.2)\n","P_overall = wp['dcr']*P_DCR + wp['qd']*P_Qdelta + wp['dup']*P_noDup\n","\n","# ---------- report ----------\n","report = pd.DataFrame([{\n","    \"dataset\": Path(real_path).name,\n","    \"synthetic\": Path(syn_path).name,\n","    \"Q_overall\": round(Q_overall,3),\n","    \"Q_num(1-KS)\": round(Q_num,3),\n","    \"Q_cat(1-TVD)\": round(Q_cat,3),\n","    \"Q_range\": round(Q_range,3),\n","    \"Q_anomaly\": round(Q_anom,3),\n","    \"Q_corr\": round(Q_corr,3),\n","    \"Q_diversity\": round(Q_div,3),\n","    \"P_overall\": round(P_overall,3),\n","    \"P_DCR\": round(P_DCR,3),\n","    \"P_Qdelta\": round(P_Qdelta,3),\n","    \"P_noDup\": round(P_noDup,3),\n","    \"n_real\": len(real),\n","    \"n_syn\": len(syn),\n","    \"num_cols_used\": len(num_cols),\n","    \"cat_cols_used\": len(cat_cols)\n","}])\n","\n","# Wide (original) ‚Äì still saved to CSV\n","report.to_csv(out_csv, index=False)\n","print(\"\\nSaved report (wide format) to:\", out_csv)\n","\n","# ---------- vertical tabular print ----------\n","vertical = report.T.reset_index()\n","vertical.columns = [\"Metric\", \"Value\"]\n","print(\"\\n==== Metrics (vertical view) ====\\n\")\n","print(vertical.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iny8tXgjRIfb","executionInfo":{"status":"ok","timestamp":1765219281431,"user_tz":480,"elapsed":2574,"user":{"displayName":"Rashmi Singapura Manjunath","userId":"03442006476881626350"}},"outputId":"523ae39b-beea-4450-d4ae-29cb8e8e9959"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Saved report (wide format) to: /content/drive/MyDrive/CTAB-GAN-Plus-main/CTAB-GAN-Plus-main/data/breast_cancer_CTABGAN_DP_QP_report.csv\n","\n","==== Metrics (vertical view) ====\n","\n","       Metric                                  Value\n","      dataset            breast-cancer-wisconsin.csv\n","    synthetic breast-cancer-wisconsin_CTABGAN_DP.csv\n","    Q_overall                                  0.814\n","  Q_num(1-KS)                                  0.754\n"," Q_cat(1-TVD)                                    1.0\n","      Q_range                                  0.953\n","    Q_anomaly                                  0.958\n","       Q_corr                                  0.382\n","  Q_diversity                                    1.0\n","    P_overall                                  0.866\n","        P_DCR                                    1.0\n","     P_Qdelta                                  0.555\n","      P_noDup                                    1.0\n","       n_real                                    683\n","        n_syn                                    683\n","num_cols_used                                     11\n","cat_cols_used                                      0\n"]}]}]}